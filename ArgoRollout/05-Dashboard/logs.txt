* 
* ==> Audit <==
* |---------|--------------------------------|----------|--------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |  User  | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|--------|---------|---------------------|---------------------|
| start   |                                | minikube | raghib | v1.32.0 | 14 Mar 24 22:15 IST |                     |
| start   |                                | minikube | raghib | v1.32.0 | 21 Mar 24 15:41 IST |                     |
| start   |                                | minikube | raghib | v1.32.0 | 21 Mar 24 15:42 IST | 21 Mar 24 15:44 IST |
| stop    |                                | minikube | raghib | v1.32.0 | 21 Mar 24 19:31 IST | 21 Mar 24 19:31 IST |
| start   |                                | minikube | raghib | v1.32.0 | 21 Mar 24 22:12 IST | 21 Mar 24 22:13 IST |
| start   |                                | minikube | raghib | v1.32.0 | 23 Mar 24 13:44 IST | 23 Mar 24 13:45 IST |
| start   |                                | minikube | raghib | v1.32.0 | 11 Apr 24 00:24 IST | 11 Apr 24 00:25 IST |
| start   |                                | minikube | raghib | v1.32.0 | 12 Apr 24 00:54 IST | 12 Apr 24 00:55 IST |
| service | dev101-mychart1 --url          | minikube | raghib | v1.32.0 | 12 Apr 24 00:59 IST | 12 Apr 24 00:59 IST |
|         | http://localhost:31231         |          |        |         |                     |                     |
| start   |                                | minikube | raghib | v1.32.0 | 30 Apr 24 13:39 IST | 30 Apr 24 13:40 IST |
| start   |                                | minikube | raghib | v1.32.0 | 05 May 24 15:12 IST | 05 May 24 15:13 IST |
| addons  | list                           | minikube | raghib | v1.32.0 | 05 May 24 21:56 IST | 05 May 24 21:56 IST |
| addons  | enable metrics-server          | minikube | raghib | v1.32.0 | 05 May 24 21:57 IST | 05 May 24 21:57 IST |
| addons  | list                           | minikube | raghib | v1.32.0 | 05 May 24 21:57 IST | 05 May 24 21:57 IST |
| start   |                                | minikube | raghib | v1.32.0 | 18 May 24 02:12 IST | 18 May 24 02:13 IST |
| service | argo-rollouts-server           | minikube | raghib | v1.32.0 | 18 May 24 12:47 IST |                     |
| service | argo-rollouts-server -n        | minikube | raghib | v1.32.0 | 18 May 24 12:47 IST | 18 May 24 12:47 IST |
|         | argo-rollouts                  |          |        |         |                     |                     |
| service | argo-rollouts-server -n        | minikube | raghib | v1.32.0 | 18 May 24 12:49 IST |                     |
|         | argo-rollouts                  |          |        |         |                     |                     |
| service | argo-rollouts-server -n        | minikube | raghib | v1.32.0 | 18 May 24 12:54 IST |                     |
|         | argo-rollouts                  |          |        |         |                     |                     |
| service | argo-rollouts-server -n        | minikube | raghib | v1.32.0 | 19 May 24 14:05 IST |                     |
|         | argo-rollouts                  |          |        |         |                     |                     |
| service | argo-rollouts-server -n        | minikube | raghib | v1.32.0 | 19 May 24 14:44 IST |                     |
|         | argo-rollouts                  |          |        |         |                     |                     |
| service | argo-rollouts-server -n        | minikube | raghib | v1.32.0 | 19 May 24 14:47 IST |                     |
|         | argo-rollouts --url            |          |        |         |                     |                     |
| service | argo-rollouts-server -n        | minikube | raghib | v1.32.0 | 19 May 24 14:48 IST |                     |
|         | argo-rollouts --url            |          |        |         |                     |                     |
| service | argo-rollouts-server --url     | minikube | raghib | v1.32.0 | 19 May 24 14:49 IST |                     |
|---------|--------------------------------|----------|--------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/05/18 02:12:53
Running on machine: raghib
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0518 02:12:53.034106   15082 out.go:296] Setting OutFile to fd 1 ...
I0518 02:12:53.034217   15082 out.go:348] isatty.IsTerminal(1) = true
I0518 02:12:53.034222   15082 out.go:309] Setting ErrFile to fd 2...
I0518 02:12:53.034232   15082 out.go:348] isatty.IsTerminal(2) = true
I0518 02:12:53.034422   15082 root.go:338] Updating PATH: /home/raghib/.minikube/bin
W0518 02:12:53.034539   15082 root.go:314] Error reading config file at /home/raghib/.minikube/config/config.json: open /home/raghib/.minikube/config/config.json: no such file or directory
I0518 02:12:53.036046   15082 out.go:303] Setting JSON to false
I0518 02:12:53.037242   15082 start.go:128] hostinfo: {"hostname":"raghib","uptime":348299,"bootTime":1715630274,"procs":255,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-28-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"4265f251-b34a-4022-bf23-66eb1109c831"}
I0518 02:12:53.037307   15082 start.go:138] virtualization: kvm host
I0518 02:12:53.044238   15082 out.go:177] üòÑ  minikube v1.32.0 on Ubuntu 22.04
I0518 02:12:53.058047   15082 notify.go:220] Checking for updates...
I0518 02:12:53.059457   15082 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0518 02:12:53.060361   15082 driver.go:378] Setting default libvirt URI to qemu:///system
I0518 02:12:53.103111   15082 docker.go:122] docker version: linux-26.0.0:Docker Engine - Community
I0518 02:12:53.103170   15082 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0518 02:12:53.343563   15082 lock.go:35] WriteFile acquiring /home/raghib/.minikube/last_update_check: {Name:mkd397b1dd07608e2f21c1c952042d21ef6ef962 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 02:12:53.362100   15082 out.go:177] üéâ  minikube 1.33.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.33.1
I0518 02:12:53.385285   15082 out.go:177] üí°  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0518 02:12:54.125399   15082 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.022186713s)
I0518 02:12:54.126204   15082 info.go:266] docker info: {ID:fe7026d3-9819-4112-be19-403948682fbc Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:40 SystemTime:2024-05-18 02:12:54.109884479 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-28-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:16595881984 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:raghib Labels:[] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.25.0]] Warnings:<nil>}}
I0518 02:12:54.126342   15082 docker.go:295] overlay module found
I0518 02:12:54.132640   15082 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0518 02:12:54.138809   15082 start.go:298] selected driver: docker
I0518 02:12:54.138821   15082 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/raghib:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0518 02:12:54.138966   15082 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0518 02:12:54.139093   15082 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0518 02:12:54.204957   15082 info.go:266] docker info: {ID:fe7026d3-9819-4112-be19-403948682fbc Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:40 SystemTime:2024-05-18 02:12:54.195887363 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-28-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:16595881984 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:raghib Labels:[] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.25.0]] Warnings:<nil>}}
I0518 02:12:54.205623   15082 cni.go:84] Creating CNI manager for ""
I0518 02:12:54.205640   15082 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0518 02:12:54.205650   15082 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/raghib:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0518 02:12:54.212852   15082 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0518 02:12:54.218607   15082 cache.go:121] Beginning downloading kic base image for docker with docker
I0518 02:12:54.224730   15082 out.go:177] üöú  Pulling base image ...
I0518 02:12:54.236623   15082 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0518 02:12:54.237281   15082 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0518 02:12:54.237390   15082 preload.go:148] Found local preload: /home/raghib/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0518 02:12:54.237404   15082 cache.go:56] Caching tarball of preloaded images
I0518 02:12:54.237860   15082 preload.go:174] Found /home/raghib/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0518 02:12:54.237904   15082 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0518 02:12:54.238200   15082 profile.go:148] Saving config to /home/raghib/.minikube/profiles/minikube/config.json ...
I0518 02:12:54.282610   15082 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0518 02:12:54.282633   15082 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0518 02:12:54.282648   15082 cache.go:194] Successfully downloaded all kic artifacts
I0518 02:12:54.282681   15082 start.go:365] acquiring machines lock for minikube: {Name:mkca2a87d05e8874716b2d35caf38c426254cd1b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0518 02:12:54.282792   15082 start.go:369] acquired machines lock for "minikube" in 94.646¬µs
I0518 02:12:54.282810   15082 start.go:96] Skipping create...Using existing machine configuration
I0518 02:12:54.282815   15082 fix.go:54] fixHost starting: 
I0518 02:12:54.283148   15082 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 02:12:54.300634   15082 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0518 02:12:54.300658   15082 fix.go:128] unexpected machine state, will restart: <nil>
I0518 02:12:54.307576   15082 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0518 02:12:54.320004   15082 cli_runner.go:164] Run: docker start minikube
I0518 02:12:55.005383   15082 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 02:12:55.038012   15082 kic.go:430] container "minikube" state is running.
I0518 02:12:55.038378   15082 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0518 02:12:55.054569   15082 profile.go:148] Saving config to /home/raghib/.minikube/profiles/minikube/config.json ...
I0518 02:12:55.054769   15082 machine.go:88] provisioning docker machine ...
I0518 02:12:55.054801   15082 ubuntu.go:169] provisioning hostname "minikube"
I0518 02:12:55.054841   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:12:55.071188   15082 main.go:141] libmachine: Using SSH client type: native
I0518 02:12:55.072895   15082 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0518 02:12:55.072903   15082 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0518 02:12:55.073415   15082 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:39548->127.0.0.1:32772: read: connection reset by peer
I0518 02:12:58.402618   15082 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0518 02:12:58.402783   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:12:58.428532   15082 main.go:141] libmachine: Using SSH client type: native
I0518 02:12:58.428998   15082 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0518 02:12:58.429023   15082 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0518 02:12:58.550802   15082 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0518 02:12:58.550834   15082 ubuntu.go:175] set auth options {CertDir:/home/raghib/.minikube CaCertPath:/home/raghib/.minikube/certs/ca.pem CaPrivateKeyPath:/home/raghib/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/raghib/.minikube/machines/server.pem ServerKeyPath:/home/raghib/.minikube/machines/server-key.pem ClientKeyPath:/home/raghib/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/raghib/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/raghib/.minikube}
I0518 02:12:58.550855   15082 ubuntu.go:177] setting up certificates
I0518 02:12:58.550864   15082 provision.go:83] configureAuth start
I0518 02:12:58.550921   15082 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0518 02:12:58.568113   15082 provision.go:138] copyHostCerts
I0518 02:12:58.568509   15082 exec_runner.go:144] found /home/raghib/.minikube/cert.pem, removing ...
I0518 02:12:58.568542   15082 exec_runner.go:203] rm: /home/raghib/.minikube/cert.pem
I0518 02:12:58.568584   15082 exec_runner.go:151] cp: /home/raghib/.minikube/certs/cert.pem --> /home/raghib/.minikube/cert.pem (1119 bytes)
I0518 02:12:58.569906   15082 exec_runner.go:144] found /home/raghib/.minikube/key.pem, removing ...
I0518 02:12:58.569910   15082 exec_runner.go:203] rm: /home/raghib/.minikube/key.pem
I0518 02:12:58.569935   15082 exec_runner.go:151] cp: /home/raghib/.minikube/certs/key.pem --> /home/raghib/.minikube/key.pem (1675 bytes)
I0518 02:12:58.570259   15082 exec_runner.go:144] found /home/raghib/.minikube/ca.pem, removing ...
I0518 02:12:58.570264   15082 exec_runner.go:203] rm: /home/raghib/.minikube/ca.pem
I0518 02:12:58.570290   15082 exec_runner.go:151] cp: /home/raghib/.minikube/certs/ca.pem --> /home/raghib/.minikube/ca.pem (1078 bytes)
I0518 02:12:58.570691   15082 provision.go:112] generating server cert: /home/raghib/.minikube/machines/server.pem ca-key=/home/raghib/.minikube/certs/ca.pem private-key=/home/raghib/.minikube/certs/ca-key.pem org=raghib.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0518 02:12:58.652769   15082 provision.go:172] copyRemoteCerts
I0518 02:12:58.652843   15082 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0518 02:12:58.652875   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:12:58.672880   15082 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/raghib/.minikube/machines/minikube/id_rsa Username:docker}
I0518 02:12:58.786297   15082 ssh_runner.go:362] scp /home/raghib/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0518 02:12:58.873909   15082 ssh_runner.go:362] scp /home/raghib/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I0518 02:12:58.918146   15082 ssh_runner.go:362] scp /home/raghib/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0518 02:12:58.949549   15082 provision.go:86] duration metric: configureAuth took 398.671388ms
I0518 02:12:58.949589   15082 ubuntu.go:193] setting minikube options for container-runtime
I0518 02:12:58.949802   15082 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0518 02:12:58.949844   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:12:58.967917   15082 main.go:141] libmachine: Using SSH client type: native
I0518 02:12:58.968287   15082 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0518 02:12:58.968299   15082 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0518 02:12:59.114639   15082 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0518 02:12:59.114683   15082 ubuntu.go:71] root file system type: overlay
I0518 02:12:59.115136   15082 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0518 02:12:59.115351   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:12:59.178786   15082 main.go:141] libmachine: Using SSH client type: native
I0518 02:12:59.180111   15082 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0518 02:12:59.180830   15082 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0518 02:12:59.393892   15082 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0518 02:12:59.393951   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:12:59.410070   15082 main.go:141] libmachine: Using SSH client type: native
I0518 02:12:59.410418   15082 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0518 02:12:59.410451   15082 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0518 02:12:59.607446   15082 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0518 02:12:59.607475   15082 machine.go:91] provisioned docker machine in 4.552696743s
I0518 02:12:59.607488   15082 start.go:300] post-start starting for "minikube" (driver="docker")
I0518 02:12:59.607504   15082 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0518 02:12:59.607570   15082 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0518 02:12:59.607631   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:12:59.630667   15082 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/raghib/.minikube/machines/minikube/id_rsa Username:docker}
I0518 02:12:59.730891   15082 ssh_runner.go:195] Run: cat /etc/os-release
I0518 02:12:59.737302   15082 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0518 02:12:59.737367   15082 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0518 02:12:59.737395   15082 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0518 02:12:59.737405   15082 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0518 02:12:59.737429   15082 filesync.go:126] Scanning /home/raghib/.minikube/addons for local assets ...
I0518 02:12:59.737895   15082 filesync.go:126] Scanning /home/raghib/.minikube/files for local assets ...
I0518 02:12:59.738182   15082 start.go:303] post-start completed in 130.684535ms
I0518 02:12:59.738245   15082 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0518 02:12:59.738301   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:12:59.765361   15082 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/raghib/.minikube/machines/minikube/id_rsa Username:docker}
I0518 02:12:59.879228   15082 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0518 02:12:59.896050   15082 fix.go:56] fixHost completed within 5.613218044s
I0518 02:12:59.896078   15082 start.go:83] releasing machines lock for "minikube", held for 5.613270423s
I0518 02:12:59.896225   15082 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0518 02:12:59.928235   15082 ssh_runner.go:195] Run: cat /version.json
I0518 02:12:59.928285   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:12:59.928327   15082 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0518 02:12:59.928404   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:12:59.945587   15082 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/raghib/.minikube/machines/minikube/id_rsa Username:docker}
I0518 02:12:59.946091   15082 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/raghib/.minikube/machines/minikube/id_rsa Username:docker}
I0518 02:13:00.489275   15082 ssh_runner.go:195] Run: systemctl --version
I0518 02:13:00.514072   15082 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0518 02:13:00.530668   15082 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0518 02:13:00.583698   15082 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0518 02:13:00.583790   15082 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0518 02:13:00.600295   15082 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0518 02:13:00.600315   15082 start.go:472] detecting cgroup driver to use...
I0518 02:13:00.600344   15082 detect.go:199] detected "systemd" cgroup driver on host os
I0518 02:13:00.600445   15082 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0518 02:13:00.622580   15082 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0518 02:13:00.634041   15082 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0518 02:13:00.645360   15082 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I0518 02:13:00.645410   15082 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0518 02:13:00.656889   15082 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0518 02:13:00.668073   15082 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0518 02:13:00.679227   15082 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0518 02:13:00.690269   15082 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0518 02:13:00.700931   15082 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0518 02:13:00.711766   15082 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0518 02:13:00.723134   15082 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0518 02:13:00.732665   15082 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 02:13:00.864706   15082 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0518 02:13:01.008972   15082 start.go:472] detecting cgroup driver to use...
I0518 02:13:01.009003   15082 detect.go:199] detected "systemd" cgroup driver on host os
I0518 02:13:01.009050   15082 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0518 02:13:01.031860   15082 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0518 02:13:01.031913   15082 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0518 02:13:01.050889   15082 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0518 02:13:01.073254   15082 ssh_runner.go:195] Run: which cri-dockerd
I0518 02:13:01.077143   15082 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0518 02:13:01.089231   15082 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0518 02:13:01.113526   15082 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0518 02:13:01.257169   15082 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0518 02:13:01.371710   15082 docker.go:560] configuring docker to use "systemd" as cgroup driver...
I0518 02:13:01.371793   15082 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0518 02:13:01.392167   15082 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 02:13:01.565155   15082 ssh_runner.go:195] Run: sudo systemctl restart docker
I0518 02:13:03.453244   15082 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.888066153s)
I0518 02:13:03.453301   15082 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0518 02:13:03.575266   15082 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0518 02:13:03.679623   15082 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0518 02:13:03.776770   15082 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 02:13:03.892362   15082 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0518 02:13:03.931150   15082 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0518 02:13:04.031718   15082 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0518 02:13:04.534721   15082 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0518 02:13:04.534832   15082 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0518 02:13:04.539772   15082 start.go:540] Will wait 60s for crictl version
I0518 02:13:04.539826   15082 ssh_runner.go:195] Run: which crictl
I0518 02:13:04.544569   15082 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0518 02:13:04.843716   15082 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0518 02:13:04.843766   15082 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0518 02:13:05.060910   15082 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0518 02:13:05.102138   15082 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0518 02:13:05.103608   15082 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0518 02:13:05.122230   15082 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0518 02:13:05.126896   15082 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0518 02:13:05.143172   15082 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0518 02:13:05.143211   15082 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0518 02:13:05.170098   15082 docker.go:671] Got preloaded images: -- stdout --
bitnami/nginx:1.25.4-debian-12-r7
quay.io/prometheus-operator/prometheus-config-reloader:v0.73.0
quay.io/prometheus-operator/prometheus-operator:v0.73.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
quay.io/prometheus/prometheus:v2.51.1
quay.io/prometheus/prometheus:v2.51.0
quay.io/argoproj/argocd:v2.10.4
quay.io/kiwigrid/k8s-sidecar:1.26.1
grafana/grafana:10.4.0
quay.io/prometheus-operator/prometheus-config-reloader:v0.72.0
quay.io/prometheus-operator/prometheus-operator:v0.72.0
quay.io/prometheus/alertmanager:v0.27.0
quay.io/argoproj/argo-rollouts:v1.6.6
redis:7.0.14-alpine
quay.io/prometheus/node-exporter:v1.7.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/metrics-server/metrics-server:<none>
ghcr.io/dexidp/dex:v2.37.0
busybox:latest
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6
sanjeevkt720/prometheus-demo:latest
registry.k8s.io/pause:3.9
argoproj/rollouts-demo:blue
argoproj/rollouts-demo:red
nginx:1.19.10
nginx:1.19.9
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.19.1
nginx:1.19.0
ghcr.io/stacksimplify/kubenginx:4.0.0
ghcr.io/stacksimplify/kubenginx:3.0.0
ghcr.io/stacksimplify/kubenginx:2.0.0
ghcr.io/stacksimplify/kubenginx:1.0.0
busybox:1.28
gcr.io/heptio-images/ks-guestbook-demo:0.2

-- /stdout --
I0518 02:13:05.170116   15082 docker.go:601] Images already preloaded, skipping extraction
I0518 02:13:05.170172   15082 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0518 02:13:05.196809   15082 docker.go:671] Got preloaded images: -- stdout --
bitnami/nginx:1.25.4-debian-12-r7
quay.io/prometheus-operator/prometheus-config-reloader:v0.73.0
quay.io/prometheus-operator/prometheus-operator:v0.73.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
quay.io/prometheus/prometheus:v2.51.1
quay.io/prometheus/prometheus:v2.51.0
quay.io/argoproj/argocd:v2.10.4
quay.io/kiwigrid/k8s-sidecar:1.26.1
grafana/grafana:10.4.0
quay.io/prometheus-operator/prometheus-config-reloader:v0.72.0
quay.io/prometheus-operator/prometheus-operator:v0.72.0
quay.io/prometheus/alertmanager:v0.27.0
quay.io/argoproj/argo-rollouts:v1.6.6
redis:7.0.14-alpine
quay.io/prometheus/node-exporter:v1.7.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/metrics-server/metrics-server:<none>
ghcr.io/dexidp/dex:v2.37.0
busybox:latest
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6
sanjeevkt720/prometheus-demo:latest
registry.k8s.io/pause:3.9
argoproj/rollouts-demo:red
argoproj/rollouts-demo:blue
nginx:1.19.10
nginx:1.19.9
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.19.1
nginx:1.19.0
ghcr.io/stacksimplify/kubenginx:4.0.0
ghcr.io/stacksimplify/kubenginx:3.0.0
ghcr.io/stacksimplify/kubenginx:2.0.0
ghcr.io/stacksimplify/kubenginx:1.0.0
busybox:1.28
gcr.io/heptio-images/ks-guestbook-demo:0.2

-- /stdout --
I0518 02:13:05.196824   15082 cache_images.go:84] Images are preloaded, skipping loading
I0518 02:13:05.196868   15082 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0518 02:13:05.812739   15082 cni.go:84] Creating CNI manager for ""
I0518 02:13:05.812767   15082 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0518 02:13:05.814300   15082 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0518 02:13:05.814360   15082 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0518 02:13:05.814674   15082 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0518 02:13:05.814861   15082 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0518 02:13:05.814941   15082 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0518 02:13:05.837873   15082 binaries.go:44] Found k8s binaries, skipping transfer
I0518 02:13:05.837962   15082 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0518 02:13:05.854135   15082 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0518 02:13:05.875078   15082 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0518 02:13:05.894630   15082 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0518 02:13:05.916035   15082 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0518 02:13:05.919693   15082 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0518 02:13:05.932081   15082 certs.go:56] Setting up /home/raghib/.minikube/profiles/minikube for IP: 192.168.49.2
I0518 02:13:05.932103   15082 certs.go:190] acquiring lock for shared ca certs: {Name:mkd366aac79a440ddcec8628f72738aa4e0f1a2a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 02:13:05.933032   15082 certs.go:199] skipping minikubeCA CA generation: /home/raghib/.minikube/ca.key
I0518 02:13:05.933441   15082 certs.go:199] skipping proxyClientCA CA generation: /home/raghib/.minikube/proxy-client-ca.key
I0518 02:13:05.933758   15082 certs.go:315] skipping minikube-user signed cert generation: /home/raghib/.minikube/profiles/minikube/client.key
I0518 02:13:05.933957   15082 certs.go:315] skipping minikube signed cert generation: /home/raghib/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0518 02:13:05.934125   15082 certs.go:315] skipping aggregator signed cert generation: /home/raghib/.minikube/profiles/minikube/proxy-client.key
I0518 02:13:05.934318   15082 certs.go:437] found cert: /home/raghib/.minikube/certs/home/raghib/.minikube/certs/ca-key.pem (1679 bytes)
I0518 02:13:05.934386   15082 certs.go:437] found cert: /home/raghib/.minikube/certs/home/raghib/.minikube/certs/ca.pem (1078 bytes)
I0518 02:13:05.934446   15082 certs.go:437] found cert: /home/raghib/.minikube/certs/home/raghib/.minikube/certs/cert.pem (1119 bytes)
I0518 02:13:05.934499   15082 certs.go:437] found cert: /home/raghib/.minikube/certs/home/raghib/.minikube/certs/key.pem (1675 bytes)
I0518 02:13:05.935725   15082 ssh_runner.go:362] scp /home/raghib/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0518 02:13:05.966591   15082 ssh_runner.go:362] scp /home/raghib/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0518 02:13:05.994703   15082 ssh_runner.go:362] scp /home/raghib/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0518 02:13:06.023317   15082 ssh_runner.go:362] scp /home/raghib/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0518 02:13:06.053966   15082 ssh_runner.go:362] scp /home/raghib/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0518 02:13:06.082032   15082 ssh_runner.go:362] scp /home/raghib/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0518 02:13:06.109525   15082 ssh_runner.go:362] scp /home/raghib/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0518 02:13:06.138539   15082 ssh_runner.go:362] scp /home/raghib/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0518 02:13:06.169033   15082 ssh_runner.go:362] scp /home/raghib/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0518 02:13:06.200777   15082 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0518 02:13:06.222057   15082 ssh_runner.go:195] Run: openssl version
I0518 02:13:06.236145   15082 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0518 02:13:06.258332   15082 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0518 02:13:06.263159   15082 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Mar 21 10:14 /usr/share/ca-certificates/minikubeCA.pem
I0518 02:13:06.263241   15082 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0518 02:13:06.271992   15082 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0518 02:13:06.283494   15082 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0518 02:13:06.288223   15082 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0518 02:13:06.295921   15082 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0518 02:13:06.304032   15082 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0518 02:13:06.311437   15082 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0518 02:13:06.318371   15082 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0518 02:13:06.325707   15082 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0518 02:13:06.332796   15082 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/raghib:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0518 02:13:06.332890   15082 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0518 02:13:06.354249   15082 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0518 02:13:06.365514   15082 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0518 02:13:06.365534   15082 kubeadm.go:636] restartCluster start
I0518 02:13:06.365578   15082 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0518 02:13:06.375396   15082 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0518 02:13:06.376894   15082 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0518 02:13:06.402117   15082 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0518 02:13:06.413225   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:06.413269   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:06.427213   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:06.427220   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:06.427255   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:06.440498   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:06.940726   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:06.941053   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:06.984146   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:07.441552   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:07.441756   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:07.484706   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:07.941211   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:07.941427   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:07.983206   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:08.441373   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:08.441538   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:08.483903   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:08.941382   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:08.941603   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:08.983165   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:09.440715   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:09.440773   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:09.455134   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:09.941263   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:09.941660   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:09.984390   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:10.441240   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:10.441459   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:10.484331   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:10.940842   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:10.941055   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:10.986678   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:11.441598   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:11.441773   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:11.484549   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:11.940908   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:11.941223   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:11.987457   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:12.441445   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:12.441694   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:12.481831   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:12.941214   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:12.941545   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:12.982168   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:13.440784   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:13.440975   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:13.479839   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:13.941442   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:13.941617   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:13.982119   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:14.441500   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:14.441556   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:14.457930   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:14.940987   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:14.941215   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:14.983403   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:15.440936   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:15.441013   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:15.463168   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:15.941315   15082 api_server.go:166] Checking apiserver status ...
I0518 02:13:15.941380   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0518 02:13:15.961261   15082 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0518 02:13:16.414049   15082 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0518 02:13:16.414090   15082 kubeadm.go:1128] stopping kube-system containers ...
I0518 02:13:16.414173   15082 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0518 02:13:16.464579   15082 docker.go:469] Stopping containers: [c9f5677c93cf 92db28148688 310f3d160b37 69ec33358294 6441d0062b9e 08eae1d8fb05 f05486398f34 7d963aa132c6 dae3f4db9b6b 31a1fe5caa19 622ab4da3827 1fc1c1e587b8 d0b0f9408e2f d9189eeebd7b 43c37b19dab0 e5b52c6259b8 428af1555ea3 ad26c722d7a9 eea905d79754 63caac9f6243 831ac387af31 848c492e1235 0d17b040365a e06fbcf18d8b bce471bd9ccb ef35f3da4d77 79070700145c 4793e8f833bd a2163da79350]
I0518 02:13:16.464688   15082 ssh_runner.go:195] Run: docker stop c9f5677c93cf 92db28148688 310f3d160b37 69ec33358294 6441d0062b9e 08eae1d8fb05 f05486398f34 7d963aa132c6 dae3f4db9b6b 31a1fe5caa19 622ab4da3827 1fc1c1e587b8 d0b0f9408e2f d9189eeebd7b 43c37b19dab0 e5b52c6259b8 428af1555ea3 ad26c722d7a9 eea905d79754 63caac9f6243 831ac387af31 848c492e1235 0d17b040365a e06fbcf18d8b bce471bd9ccb ef35f3da4d77 79070700145c 4793e8f833bd a2163da79350
I0518 02:13:16.504696   15082 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0518 02:13:16.526232   15082 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0518 02:13:16.541992   15082 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Mar 21 10:14 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 May  5 09:42 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Mar 21 10:14 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 May  5 09:42 /etc/kubernetes/scheduler.conf

I0518 02:13:16.542034   15082 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0518 02:13:16.559029   15082 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0518 02:13:16.570555   15082 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0518 02:13:16.581287   15082 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0518 02:13:16.581340   15082 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0518 02:13:16.591500   15082 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0518 02:13:16.602447   15082 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0518 02:13:16.602523   15082 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0518 02:13:16.612560   15082 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0518 02:13:16.622985   15082 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0518 02:13:16.623004   15082 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0518 02:13:17.099903   15082 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0518 02:13:17.808975   15082 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0518 02:13:18.023214   15082 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0518 02:13:18.079557   15082 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0518 02:13:18.138359   15082 api_server.go:52] waiting for apiserver process to appear ...
I0518 02:13:18.138441   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 02:13:18.152759   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 02:13:18.666140   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 02:13:19.165802   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 02:13:19.665854   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 02:13:20.166163   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 02:13:20.665667   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 02:13:21.165672   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 02:13:21.665559   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 02:13:21.697401   15082 api_server.go:72] duration metric: took 3.559037892s to wait for apiserver process to appear ...
I0518 02:13:21.697420   15082 api_server.go:88] waiting for apiserver healthz status ...
I0518 02:13:21.697439   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:21.697784   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:21.697805   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:21.698069   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:22.198903   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:22.199886   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:22.698260   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:22.699232   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:23.198478   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:23.199644   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:23.698600   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:23.699658   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:24.198624   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:24.199572   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:24.699143   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:24.699442   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:25.198841   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:25.199167   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:25.698525   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:25.698814   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:26.198996   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:26.200000   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:26.698523   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:26.698781   15082 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0518 02:13:27.198654   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:31.201650   15082 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0518 02:13:31.201667   15082 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0518 02:13:31.201678   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:31.284131   15082 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0518 02:13:31.284147   15082 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0518 02:13:31.698703   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:31.703831   15082 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0518 02:13:31.703853   15082 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0518 02:13:32.198140   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:32.251512   15082 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0518 02:13:32.251561   15082 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0518 02:13:32.698727   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:32.713517   15082 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0518 02:13:32.713565   15082 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0518 02:13:33.198830   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:33.206226   15082 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0518 02:13:33.206252   15082 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0518 02:13:33.698812   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:33.705441   15082 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0518 02:13:33.717687   15082 api_server.go:141] control plane version: v1.28.3
I0518 02:13:33.717706   15082 api_server.go:131] duration metric: took 12.020277015s to wait for apiserver health ...
I0518 02:13:33.717714   15082 cni.go:84] Creating CNI manager for ""
I0518 02:13:33.717728   15082 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0518 02:13:33.723974   15082 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0518 02:13:33.730399   15082 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0518 02:13:33.755809   15082 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0518 02:13:33.792212   15082 system_pods.go:43] waiting for kube-system pods to appear ...
I0518 02:13:33.808548   15082 system_pods.go:59] 8 kube-system pods found
I0518 02:13:33.808568   15082 system_pods.go:61] "coredns-5dd5756b68-rjtb2" [07bf09a3-fac5-4e08-b423-3a0ed27c19f3] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0518 02:13:33.808581   15082 system_pods.go:61] "etcd-minikube" [04b0811a-f6f2-4c82-b6bd-0eb7eb1931da] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0518 02:13:33.808593   15082 system_pods.go:61] "kube-apiserver-minikube" [b5c3bb18-e5a4-4999-8773-8b1cd950f163] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0518 02:13:33.808602   15082 system_pods.go:61] "kube-controller-manager-minikube" [9a2b8290-23ec-4ba7-834b-8a6f9fa8fe07] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0518 02:13:33.808610   15082 system_pods.go:61] "kube-proxy-42gnw" [3b098a2d-5ee3-4d59-8aab-fcabd84c9bac] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0518 02:13:33.808616   15082 system_pods.go:61] "kube-scheduler-minikube" [483a9c27-8af3-42df-a759-170dc01332b9] Running
I0518 02:13:33.808624   15082 system_pods.go:61] "metrics-server-7c66d45ddc-nlf8d" [9ce9e06f-b451-4479-b64e-dbe5252a73d2] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0518 02:13:33.808638   15082 system_pods.go:61] "storage-provisioner" [a673b58f-174a-4df9-acb3-b76666ecc6a7] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0518 02:13:33.808645   15082 system_pods.go:74] duration metric: took 16.403394ms to wait for pod list to return data ...
I0518 02:13:33.808652   15082 node_conditions.go:102] verifying NodePressure condition ...
I0518 02:13:33.816264   15082 node_conditions.go:122] node storage ephemeral capacity is 244506940Ki
I0518 02:13:33.816281   15082 node_conditions.go:123] node cpu capacity is 4
I0518 02:13:33.816303   15082 node_conditions.go:105] duration metric: took 7.646794ms to run NodePressure ...
I0518 02:13:33.816321   15082 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0518 02:13:34.415497   15082 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0518 02:13:34.433275   15082 ops.go:34] apiserver oom_adj: -16
I0518 02:13:34.433287   15082 kubeadm.go:640] restartCluster took 28.067747483s
I0518 02:13:34.433294   15082 kubeadm.go:406] StartCluster complete in 28.100504324s
I0518 02:13:34.433309   15082 settings.go:142] acquiring lock: {Name:mk58f47497272123b589e4297f7ade0ed41f2b8b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 02:13:34.433414   15082 settings.go:150] Updating kubeconfig:  /home/raghib/.kube/config
I0518 02:13:34.434316   15082 lock.go:35] WriteFile acquiring /home/raghib/.kube/config: {Name:mked11ede974b5bff170d7bc3d430d46c9c54350 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0518 02:13:34.436009   15082 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0518 02:13:34.436309   15082 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0518 02:13:34.436332   15082 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0518 02:13:34.436407   15082 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0518 02:13:34.436423   15082 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0518 02:13:34.436430   15082 addons.go:240] addon storage-provisioner should already be in state true
I0518 02:13:34.436478   15082 host.go:66] Checking if "minikube" exists ...
I0518 02:13:34.436971   15082 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0518 02:13:34.436988   15082 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0518 02:13:34.437365   15082 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 02:13:34.438120   15082 addons.go:69] Setting metrics-server=true in profile "minikube"
I0518 02:13:34.438148   15082 addons.go:231] Setting addon metrics-server=true in "minikube"
W0518 02:13:34.438156   15082 addons.go:240] addon metrics-server should already be in state true
I0518 02:13:34.438268   15082 host.go:66] Checking if "minikube" exists ...
I0518 02:13:34.439023   15082 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 02:13:34.439538   15082 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 02:13:34.440367   15082 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0518 02:13:34.440395   15082 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0518 02:13:34.457320   15082 out.go:177] üîé  Verifying Kubernetes components...
I0518 02:13:34.463634   15082 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0518 02:13:34.479324   15082 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0518 02:13:34.479337   15082 addons.go:240] addon default-storageclass should already be in state true
I0518 02:13:34.479385   15082 host.go:66] Checking if "minikube" exists ...
I0518 02:13:34.480290   15082 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0518 02:13:34.505496   15082 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0518 02:13:34.520668   15082 out.go:177]     ‚ñ™ Using image registry.k8s.io/metrics-server/metrics-server:v0.6.4
I0518 02:13:34.517647   15082 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0518 02:13:34.523199   15082 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0518 02:13:34.531762   15082 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0518 02:13:34.531864   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:13:34.531997   15082 addons.go:423] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0518 02:13:34.532010   15082 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0518 02:13:34.532066   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:13:34.532141   15082 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0518 02:13:34.532206   15082 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0518 02:13:34.586195   15082 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/raghib/.minikube/machines/minikube/id_rsa Username:docker}
I0518 02:13:34.598743   15082 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/raghib/.minikube/machines/minikube/id_rsa Username:docker}
I0518 02:13:34.599532   15082 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/raghib/.minikube/machines/minikube/id_rsa Username:docker}
I0518 02:13:34.754430   15082 addons.go:423] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0518 02:13:34.754440   15082 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0518 02:13:34.764650   15082 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0518 02:13:34.765791   15082 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0518 02:13:34.806074   15082 addons.go:423] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0518 02:13:34.806090   15082 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0518 02:13:34.849568   15082 addons.go:423] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0518 02:13:34.849581   15082 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0518 02:13:34.888860   15082 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0518 02:13:35.300907   15082 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0518 02:13:35.300943   15082 api_server.go:52] waiting for apiserver process to appear ...
I0518 02:13:35.300989   15082 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0518 02:13:52.636467   15082 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (17.871747511s)
I0518 02:13:52.636548   15082 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (17.870703018s)
I0518 02:13:52.668231   15082 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (17.779339719s)
I0518 02:13:52.668256   15082 addons.go:467] Verifying addon metrics-server=true in "minikube"
I0518 02:13:52.674588   15082 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass, metrics-server
I0518 02:13:52.668477   15082 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (17.367469272s)
I0518 02:13:52.686671   15082 addons.go:502] enable addons completed in 18.250329858s: enabled=[storage-provisioner default-storageclass metrics-server]
I0518 02:13:52.686684   15082 api_server.go:72] duration metric: took 18.24624914s to wait for apiserver process to appear ...
I0518 02:13:52.686701   15082 api_server.go:88] waiting for apiserver healthz status ...
I0518 02:13:52.686713   15082 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0518 02:13:52.691520   15082 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0518 02:13:52.692389   15082 api_server.go:141] control plane version: v1.28.3
I0518 02:13:52.692400   15082 api_server.go:131] duration metric: took 5.693464ms to wait for apiserver health ...
I0518 02:13:52.692405   15082 system_pods.go:43] waiting for kube-system pods to appear ...
I0518 02:13:52.701250   15082 system_pods.go:59] 8 kube-system pods found
I0518 02:13:52.701276   15082 system_pods.go:61] "coredns-5dd5756b68-rjtb2" [07bf09a3-fac5-4e08-b423-3a0ed27c19f3] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0518 02:13:52.701281   15082 system_pods.go:61] "etcd-minikube" [04b0811a-f6f2-4c82-b6bd-0eb7eb1931da] Running
I0518 02:13:52.701288   15082 system_pods.go:61] "kube-apiserver-minikube" [b5c3bb18-e5a4-4999-8773-8b1cd950f163] Running
I0518 02:13:52.701291   15082 system_pods.go:61] "kube-controller-manager-minikube" [9a2b8290-23ec-4ba7-834b-8a6f9fa8fe07] Running
I0518 02:13:52.701296   15082 system_pods.go:61] "kube-proxy-42gnw" [3b098a2d-5ee3-4d59-8aab-fcabd84c9bac] Running
I0518 02:13:52.701302   15082 system_pods.go:61] "kube-scheduler-minikube" [483a9c27-8af3-42df-a759-170dc01332b9] Running
I0518 02:13:52.701311   15082 system_pods.go:61] "metrics-server-7c66d45ddc-nlf8d" [9ce9e06f-b451-4479-b64e-dbe5252a73d2] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0518 02:13:52.701318   15082 system_pods.go:61] "storage-provisioner" [a673b58f-174a-4df9-acb3-b76666ecc6a7] Running
I0518 02:13:52.701325   15082 system_pods.go:74] duration metric: took 8.915598ms to wait for pod list to return data ...
I0518 02:13:52.701343   15082 kubeadm.go:581] duration metric: took 18.260903357s to wait for : map[apiserver:true system_pods:true] ...
I0518 02:13:52.701358   15082 node_conditions.go:102] verifying NodePressure condition ...
I0518 02:13:52.705947   15082 node_conditions.go:122] node storage ephemeral capacity is 244506940Ki
I0518 02:13:52.705960   15082 node_conditions.go:123] node cpu capacity is 4
I0518 02:13:52.705967   15082 node_conditions.go:105] duration metric: took 4.604522ms to run NodePressure ...
I0518 02:13:52.706001   15082 start.go:228] waiting for startup goroutines ...
I0518 02:13:52.706012   15082 start.go:233] waiting for cluster config update ...
I0518 02:13:52.706022   15082 start.go:242] writing updated cluster config ...
I0518 02:13:52.706329   15082 ssh_runner.go:195] Run: rm -f paused
I0518 02:13:53.369722   15082 start.go:600] kubectl: 1.29.3, cluster: 1.28.3 (minor skew: 1)
I0518 02:13:53.380764   15082 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* May 17 21:21:34 minikube dockerd[892]: time="2024-05-17T21:21:34.238428292Z" level=error msg="stream copy error: reading from a closed fifo"
May 17 21:25:23 minikube dockerd[892]: time="2024-05-17T21:25:23.457671566Z" level=info msg="ignoring event" container=1ba592b6f853520bc45d14b1a7348fb3d4c3a01510b77ebe35ab729908abf196 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 17 21:25:23 minikube dockerd[892]: time="2024-05-17T21:25:23.760286716Z" level=info msg="ignoring event" container=ecda97b5f3351fe53112590d20a93438cc016801120a740ffcd9d972368b7638 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 17 21:25:24 minikube cri-dockerd[1173]: time="2024-05-17T21:25:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a7df5dccdeb833bb9b4f71039a252b79261039f60b5990af059cf4da16a09b01/resolv.conf as [nameserver 10.96.0.10 search argo-rollouts.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 17 21:25:40 minikube cri-dockerd[1173]: time="2024-05-17T21:25:40Z" level=info msg="Pulling image quay.io/argoproj/argo-rollouts:latest: 2bc164f7d9a8: Downloading [===============>                                   ]   15.2MB/49.97MB"
May 17 21:25:50 minikube cri-dockerd[1173]: time="2024-05-17T21:25:50Z" level=info msg="Pulling image quay.io/argoproj/argo-rollouts:latest: 2bc164f7d9a8: Extracting [========================>                          ]  24.12MB/49.97MB"
May 17 21:25:50 minikube cri-dockerd[1173]: time="2024-05-17T21:25:50Z" level=info msg="Stop pulling image quay.io/argoproj/argo-rollouts:latest: Status: Downloaded newer image for quay.io/argoproj/argo-rollouts:latest"
May 17 21:26:41 minikube cri-dockerd[1173]: time="2024-05-17T21:26:41Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 17 21:26:42 minikube dockerd[892]: time="2024-05-17T21:26:42.116556248Z" level=error msg="stream copy error: reading from a closed fifo"
May 17 21:31:51 minikube cri-dockerd[1173]: time="2024-05-17T21:31:51Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 17 21:31:52 minikube dockerd[892]: time="2024-05-17T21:31:52.296092318Z" level=error msg="stream copy error: reading from a closed fifo"
May 17 21:37:05 minikube cri-dockerd[1173]: time="2024-05-17T21:37:05Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 17 21:37:05 minikube dockerd[892]: time="2024-05-17T21:37:05.396044894Z" level=error msg="stream copy error: reading from a closed fifo"
May 17 21:42:09 minikube cri-dockerd[1173]: time="2024-05-17T21:42:09Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 17 21:42:09 minikube dockerd[892]: time="2024-05-17T21:42:09.787766096Z" level=error msg="stream copy error: reading from a closed fifo"
May 17 21:47:16 minikube cri-dockerd[1173]: time="2024-05-17T21:47:16Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 17 21:47:17 minikube dockerd[892]: time="2024-05-17T21:47:17.395237446Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:09:59 minikube cri-dockerd[1173]: time="2024-05-18T06:09:59Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 06:09:59 minikube dockerd[892]: time="2024-05-18T06:09:59.988792418Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:15:05 minikube cri-dockerd[1173]: time="2024-05-18T06:15:05Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 06:15:05 minikube dockerd[892]: time="2024-05-18T06:15:05.430045655Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:20:15 minikube cri-dockerd[1173]: time="2024-05-18T06:20:15Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 06:20:15 minikube dockerd[892]: time="2024-05-18T06:20:15.362543362Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:25:27 minikube cri-dockerd[1173]: time="2024-05-18T06:25:27Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 06:25:28 minikube dockerd[892]: time="2024-05-18T06:25:28.240017423Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:30:36 minikube cri-dockerd[1173]: time="2024-05-18T06:30:36Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 06:30:36 minikube dockerd[892]: time="2024-05-18T06:30:36.742000407Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:35:48 minikube cri-dockerd[1173]: time="2024-05-18T06:35:48Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 06:35:48 minikube dockerd[892]: time="2024-05-18T06:35:48.875646346Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:40:53 minikube cri-dockerd[1173]: time="2024-05-18T06:40:53Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 06:40:54 minikube dockerd[892]: time="2024-05-18T06:40:54.438276715Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:45:59 minikube cri-dockerd[1173]: time="2024-05-18T06:45:59Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 06:45:59 minikube dockerd[892]: time="2024-05-18T06:45:59.452460325Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:51:10 minikube cri-dockerd[1173]: time="2024-05-18T06:51:10Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 06:51:11 minikube dockerd[892]: time="2024-05-18T06:51:11.004960903Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:51:23 minikube cri-dockerd[1173]: time="2024-05-18T06:51:23Z" level=error msg="Error response from daemon: No such container: c8693566e1cf4fdbe074ccfe0d1e13578864d724ae4f593ad09f24c9a56709cb Failed to get stats from container c8693566e1cf4fdbe074ccfe0d1e13578864d724ae4f593ad09f24c9a56709cb"
May 18 06:51:24 minikube cri-dockerd[1173]: time="2024-05-18T06:51:24Z" level=error msg="Error response from daemon: No such container: c8693566e1cf4fdbe074ccfe0d1e13578864d724ae4f593ad09f24c9a56709cb Failed to get stats from container c8693566e1cf4fdbe074ccfe0d1e13578864d724ae4f593ad09f24c9a56709cb"
May 18 06:56:14 minikube cri-dockerd[1173]: time="2024-05-18T06:56:14Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 06:56:15 minikube dockerd[892]: time="2024-05-18T06:56:15.065143988Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 06:56:24 minikube dockerd[892]: time="2024-05-18T06:56:24.043881759Z" level=error msg="Failed to compute size of container rootfs d0b301fb389d5586d7e7713cf2baebcdcdf2a5641d13eca592ad25f8b8e0ec3f: mount does not exist"
May 18 06:56:24 minikube cri-dockerd[1173]: time="2024-05-18T06:56:24Z" level=error msg="Error response from daemon: No such container: d0b301fb389d5586d7e7713cf2baebcdcdf2a5641d13eca592ad25f8b8e0ec3f Failed to get stats from container d0b301fb389d5586d7e7713cf2baebcdcdf2a5641d13eca592ad25f8b8e0ec3f"
May 18 07:01:23 minikube cri-dockerd[1173]: time="2024-05-18T07:01:23Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 18 07:01:23 minikube dockerd[892]: time="2024-05-18T07:01:23.329414348Z" level=error msg="stream copy error: reading from a closed fifo"
May 18 07:05:35 minikube dockerd[892]: time="2024-05-18T07:05:35.275806305Z" level=info msg="ignoring event" container=8498e57bcd558057d44e5f6eb7686c621f9324f1af3caeb5196f6d0f96d25671 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 18 07:08:23 minikube cri-dockerd[1173]: time="2024-05-18T07:08:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4140c286f229de42781b0e53fa8d3541d26edcf133747e92da2f1b95e872a1e5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 07:08:39 minikube cri-dockerd[1173]: time="2024-05-18T07:08:39Z" level=info msg="Pulling image quay.io/argoproj/kubectl-argo-rollouts:latest: f02a0e02d646: Downloading [===============>                                   ]  15.87MB/50.59MB"
May 18 07:08:49 minikube cri-dockerd[1173]: time="2024-05-18T07:08:49Z" level=info msg="Pulling image quay.io/argoproj/kubectl-argo-rollouts:latest: f02a0e02d646: Extracting [=================>                                 ]   17.3MB/50.59MB"
May 18 07:08:50 minikube cri-dockerd[1173]: time="2024-05-18T07:08:50Z" level=info msg="Stop pulling image quay.io/argoproj/kubectl-argo-rollouts:latest: Status: Downloaded newer image for quay.io/argoproj/kubectl-argo-rollouts:latest"
May 18 07:10:48 minikube cri-dockerd[1173]: time="2024-05-18T07:10:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/93f53a307b771db5b1430c1dde3807be8646681b2031c3c6d40725083011028f/resolv.conf as [nameserver 10.96.0.10 search argo-rollouts.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 18 07:10:53 minikube cri-dockerd[1173]: time="2024-05-18T07:10:53Z" level=info msg="Stop pulling image quay.io/argoproj/kubectl-argo-rollouts:latest: Status: Image is up to date for quay.io/argoproj/kubectl-argo-rollouts:latest"
May 19 08:34:54 minikube dockerd[892]: time="2024-05-19T08:34:54.675138673Z" level=info msg="ignoring event" container=fac083a57b963319ea7bc7d0ffba56beab1f9c5ae60065ff445307bbbda63ab7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 19 08:34:54 minikube dockerd[892]: time="2024-05-19T08:34:54.902081448Z" level=info msg="ignoring event" container=4140c286f229de42781b0e53fa8d3541d26edcf133747e92da2f1b95e872a1e5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 19 08:55:04 minikube dockerd[892]: time="2024-05-19T08:55:04.221979867Z" level=info msg="ignoring event" container=95720f08d6d27dcbb08de598ea08caea6dc3124534717c3d5e77aed83a24ebd1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 19 08:55:04 minikube dockerd[892]: time="2024-05-19T08:55:04.576759858Z" level=info msg="ignoring event" container=93f53a307b771db5b1430c1dde3807be8646681b2031c3c6d40725083011028f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 19 09:02:57 minikube cri-dockerd[1173]: time="2024-05-19T09:02:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ae4f7e7e200128cdd27777f50f509a1934b0896880711224c4b0740998d3c505/resolv.conf as [nameserver 10.96.0.10 search argo-rollouts.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 19 09:03:01 minikube cri-dockerd[1173]: time="2024-05-19T09:03:01Z" level=info msg="Stop pulling image quay.io/argoproj/kubectl-argo-rollouts:latest: Status: Image is up to date for quay.io/argoproj/kubectl-argo-rollouts:latest"
May 19 09:18:55 minikube dockerd[892]: time="2024-05-19T09:18:55.058697005Z" level=info msg="ignoring event" container=c123f20d9906ff6322da32e6e8fa500f5638c6b960dbcbbfc8f59a4acbc9ba0b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 19 09:18:55 minikube dockerd[892]: time="2024-05-19T09:18:55.322054980Z" level=info msg="ignoring event" container=ae4f7e7e200128cdd27777f50f509a1934b0896880711224c4b0740998d3c505 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 19 09:19:20 minikube cri-dockerd[1173]: time="2024-05-19T09:19:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6744c5d6fbd8797a9b792e2ca2ba7940c2172c34bbf7d1a29acfb6e3b7b63958/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 19 09:19:24 minikube cri-dockerd[1173]: time="2024-05-19T09:19:24Z" level=info msg="Stop pulling image quay.io/argoproj/kubectl-argo-rollouts:latest: Status: Image is up to date for quay.io/argoproj/kubectl-argo-rollouts:latest"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                            CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
b91b049256cae       quay.io/argoproj/kubectl-argo-rollouts@sha256:2a3b2cb7398494f64e827f7ea94855a474498d990b76707fa1bc8cb63e7d5729   About a minute ago   Running             argo-rollouts-dashboard   0                   6744c5d6fbd87       argo-rollouts-dashboard-5577ff8545-tr74x
08966a282575e       quay.io/argoproj/argo-rollouts@sha256:df0d1ff3f16ef1b195174b725315a9ebfeef28ab0877fdccbd66b7609808556c           36 hours ago         Running             argo-rollouts             0                   a7df5dccdeb83       argo-rollouts-5fc7cd6598-qqp4w
7cd87ce0ec710       ceca2f5c4ce74                                                                                                    37 hours ago         Running             kube-prometheus-stack     4                   54d2d1fc02755       prometheus-kube-prometheus-operator-54577f779b-4txmz
7154bc9d26e72       6e38f40d628db                                                                                                    37 hours ago         Running             storage-provisioner       15                  d0fe8b2d67716       storage-provisioner
4939990bb48c1       a608c686bac93                                                                                                    37 hours ago         Running             metrics-server            2                   bc5b75e430ca6       metrics-server-7c66d45ddc-nlf8d
e79a961dc096a       b20e6a3825670                                                                                                    37 hours ago         Running             kube-state-metrics        4                   8fedb40eb1221       prometheus-kube-state-metrics-c8f945cbb-g7lm8
4a4cb25c9eeb5       f207060598a68                                                                                                    37 hours ago         Running             config-reloader           2                   426b62eb99987       prometheus-prometheus-kube-prometheus-prometheus-0
065d45c659dff       f207060598a68                                                                                                    37 hours ago         Running             config-reloader           2                   c71e162939c59       alertmanager-prometheus-kube-prometheus-alertmanager-0
8dd3727698a6d       e350b167c4fa1                                                                                                    37 hours ago         Running             prometheus                2                   426b62eb99987       prometheus-prometheus-kube-prometheus-prometheus-0
1d73253d1f006       11f11916f8cdf                                                                                                    37 hours ago         Running             alertmanager              2                   c71e162939c59       alertmanager-prometheus-kube-prometheus-alertmanager-0
c4e4a38734f0f       c8b91775d855b                                                                                                    37 hours ago         Running             grafana                   2                   1f695407f541a       prometheus-grafana-86b6d8f896-jmzxj
b0e330467176c       2e6ed1888609c                                                                                                    37 hours ago         Running             grafana-sc-datasources    2                   1f695407f541a       prometheus-grafana-86b6d8f896-jmzxj
d72f6b59361d4       ceca2f5c4ce74                                                                                                    37 hours ago         Exited              kube-prometheus-stack     3                   54d2d1fc02755       prometheus-kube-prometheus-operator-54577f779b-4txmz
0732b10d1c6ee       b20e6a3825670                                                                                                    37 hours ago         Exited              kube-state-metrics        3                   8fedb40eb1221       prometheus-kube-state-metrics-c8f945cbb-g7lm8
271b93b15f020       2e6ed1888609c                                                                                                    37 hours ago         Running             grafana-sc-dashboard      2                   1f695407f541a       prometheus-grafana-86b6d8f896-jmzxj
50bbeced86bc8       6e38f40d628db                                                                                                    37 hours ago         Exited              storage-provisioner       14                  d0fe8b2d67716       storage-provisioner
2e082df0a5f35       ead0a4a53df89                                                                                                    37 hours ago         Running             coredns                   7                   76f00ec334f27       coredns-5dd5756b68-rjtb2
85d37d6fedd65       bfc896cf80fba                                                                                                    37 hours ago         Running             kube-proxy                7                   7e54ed7ee5834       kube-proxy-42gnw
9e795c96dcb61       f207060598a68                                                                                                    37 hours ago         Exited              init-config-reloader      0                   c71e162939c59       alertmanager-prometheus-kube-prometheus-alertmanager-0
c8020a2d68bc2       72c9c20889862                                                                                                    37 hours ago         Running             node-exporter             2                   7e150df539e91       prometheus-prometheus-node-exporter-sctkf
a11a66e8885a5       a608c686bac93                                                                                                    37 hours ago         Exited              metrics-server            1                   bc5b75e430ca6       metrics-server-7c66d45ddc-nlf8d
6d256898c6235       f207060598a68                                                                                                    37 hours ago         Exited              init-config-reloader      0                   426b62eb99987       prometheus-prometheus-kube-prometheus-prometheus-0
5627bfd0b4277       5374347291230                                                                                                    37 hours ago         Running             kube-apiserver            7                   9dad5cded47c7       kube-apiserver-minikube
394ca45e182d2       73deb9a3f7025                                                                                                    37 hours ago         Running             etcd                      7                   254e740e7ae66       etcd-minikube
fc1c22bdd914b       10baa1ca17068                                                                                                    37 hours ago         Running             kube-controller-manager   7                   cc735588b04e9       kube-controller-manager-minikube
519dc8dbe10d8       6d1b4fd1b182d                                                                                                    37 hours ago         Running             kube-scheduler            7                   d2ff1258193de       kube-scheduler-minikube
3787fd141993a       f207060598a68                                                                                                    2 weeks ago          Exited              config-reloader           1                   f5d233d23eb22       prometheus-prometheus-kube-prometheus-prometheus-0
57990bed3cefd       f207060598a68                                                                                                    2 weeks ago          Exited              config-reloader           1                   7cea83c45eb20       alertmanager-prometheus-kube-prometheus-alertmanager-0
9b79386de1836       11f11916f8cdf                                                                                                    2 weeks ago          Exited              alertmanager              1                   7cea83c45eb20       alertmanager-prometheus-kube-prometheus-alertmanager-0
a0ee7a7a095f2       e350b167c4fa1                                                                                                    2 weeks ago          Exited              prometheus                1                   f5d233d23eb22       prometheus-prometheus-kube-prometheus-prometheus-0
b0d3dd74689ec       c8b91775d855b                                                                                                    2 weeks ago          Exited              grafana                   1                   8775b4c8243d2       prometheus-grafana-86b6d8f896-jmzxj
69ec33358294b       bfc896cf80fba                                                                                                    2 weeks ago          Exited              kube-proxy                6                   f05486398f344       kube-proxy-42gnw
ba2df23c78a00       72c9c20889862                                                                                                    2 weeks ago          Exited              node-exporter             1                   8e9902206dcc9       prometheus-prometheus-node-exporter-sctkf
04c1308331f73       2e6ed1888609c                                                                                                    2 weeks ago          Exited              grafana-sc-datasources    1                   8775b4c8243d2       prometheus-grafana-86b6d8f896-jmzxj
528f217d2a7ad       2e6ed1888609c                                                                                                    2 weeks ago          Exited              grafana-sc-dashboard      1                   8775b4c8243d2       prometheus-grafana-86b6d8f896-jmzxj
08eae1d8fb052       ead0a4a53df89                                                                                                    2 weeks ago          Exited              coredns                   6                   dae3f4db9b6b1       coredns-5dd5756b68-rjtb2
31a1fe5caa196       73deb9a3f7025                                                                                                    2 weeks ago          Exited              etcd                      6                   428af1555ea35       etcd-minikube
622ab4da3827b       10baa1ca17068                                                                                                    2 weeks ago          Exited              kube-controller-manager   6                   43c37b19dab00       kube-controller-manager-minikube
1fc1c1e587b87       6d1b4fd1b182d                                                                                                    2 weeks ago          Exited              kube-scheduler            6                   e5b52c6259b81       kube-scheduler-minikube
d0b0f9408e2f3       5374347291230                                                                                                    2 weeks ago          Exited              kube-apiserver            6                   d9189eeebd7bc       kube-apiserver-minikube

* 
* ==> coredns [08eae1d8fb05] <==
* [INFO] 10.244.0.142:48262 - 64035 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000296142s
[INFO] 10.244.0.142:58625 - 17488 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000388162s
[INFO] 10.244.0.142:44496 - 15624 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.00034678s
[INFO] 10.244.0.142:48315 - 36245 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000266424s
[INFO] 10.244.0.142:60830 - 51778 "AAAA IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.017243011s
[INFO] 10.244.0.142:47823 - 28224 "A IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.01348916s
[INFO] 10.244.0.142:60361 - 47145 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.023394169s
[INFO] 10.244.0.142:38805 - 4424 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.021734102s
[INFO] 10.244.0.142:60165 - 41290 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000260876s
[INFO] 10.244.0.142:58367 - 29142 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000237336s
[INFO] 10.244.0.142:45573 - 4506 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000329155s
[INFO] 10.244.0.142:37682 - 61112 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000398597s
[INFO] 10.244.0.142:37993 - 37912 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000444739s
[INFO] 10.244.0.142:45515 - 351 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000735873s
[INFO] 10.244.0.142:49958 - 64289 "AAAA IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.012194975s
[INFO] 10.244.0.142:58887 - 28036 "A IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.013835959s
[INFO] 10.244.0.142:59091 - 13049 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.030318847s
[INFO] 10.244.0.142:37661 - 32691 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.029556586s
[INFO] 10.244.0.142:49136 - 5851 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000557753s
[INFO] 10.244.0.142:45146 - 11063 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.00076887s
[INFO] 10.244.0.142:53355 - 5424 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000331932s
[INFO] 10.244.0.142:59032 - 7570 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000247922s
[INFO] 10.244.0.142:57342 - 15276 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000250368s
[INFO] 10.244.0.142:57294 - 34192 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000252398s
[INFO] 10.244.0.142:35938 - 34217 "A IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.011435835s
[INFO] 10.244.0.142:43132 - 37217 "AAAA IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.012110066s
[INFO] 10.244.0.142:45628 - 38899 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.030046776s
[INFO] 10.244.0.142:37079 - 30128 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.030874279s
[INFO] 10.244.0.142:38586 - 11223 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000635241s
[INFO] 10.244.0.142:57026 - 42106 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000869582s
[INFO] 10.244.0.142:54218 - 48915 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000425005s
[INFO] 10.244.0.142:38898 - 29488 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000348594s
[INFO] 10.244.0.142:43948 - 52507 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000468398s
[INFO] 10.244.0.142:54057 - 52300 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000648352s
[INFO] 10.244.0.142:45669 - 19609 "A IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.01438584s
[INFO] 10.244.0.142:53223 - 21557 "AAAA IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.015626244s
[INFO] 10.244.0.142:47152 - 25517 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.067970014s
[INFO] 10.244.0.142:53616 - 59925 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.090501344s
[INFO] 10.244.0.142:49153 - 8600 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000626872s
[INFO] 10.244.0.142:36863 - 36050 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000849225s
[INFO] 10.244.0.142:39548 - 9003 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.00071761s
[INFO] 10.244.0.142:40659 - 24409 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000716284s
[INFO] 10.244.0.142:60576 - 47006 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000287638s
[INFO] 10.244.0.142:48325 - 28732 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000351096s
[INFO] 10.244.0.142:34671 - 49884 "AAAA IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.081205313s
[INFO] 10.244.0.142:44046 - 64291 "A IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.082535001s
[INFO] 10.244.0.142:57405 - 681 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.068362697s
[INFO] 10.244.0.142:34293 - 60674 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.090188104s
[INFO] 10.244.0.142:33597 - 57998 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000503675s
[INFO] 10.244.0.142:40122 - 19782 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000917048s
[INFO] 10.244.0.142:39353 - 21692 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000396931s
[INFO] 10.244.0.142:50954 - 1230 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000493285s
[INFO] 10.244.0.142:39503 - 46979 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000341235s
[INFO] 10.244.0.142:58601 - 46404 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.00030174s
[INFO] 10.244.0.142:35987 - 18713 "A IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.01391845s
[INFO] 10.244.0.142:49173 - 59809 "AAAA IN grafana.com.lan. udp 44 false 1232" NXDOMAIN qr,rd,ra 33 0.015443199s
[INFO] 10.244.0.142:56753 - 9442 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.003043469s
[INFO] 10.244.0.142:59202 - 42448 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.034180213s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [2e082df0a5f3] <==
* [INFO] 10.244.0.166:59709 - 32381 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000180028s
[INFO] 10.244.0.166:41108 - 14165 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.00014041s
[INFO] 10.244.0.166:56914 - 14242 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.008865565s
[INFO] 10.244.0.166:54750 - 33866 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.009973501s
[INFO] 10.244.0.166:45609 - 47418 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000176992s
[INFO] 10.244.0.166:43925 - 44913 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000221905s
[INFO] 10.244.0.166:56297 - 56892 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000130805s
[INFO] 10.244.0.166:37384 - 60636 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000107771s
[INFO] 10.244.0.166:45541 - 56720 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000117914s
[INFO] 10.244.0.166:42824 - 62356 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000107338s
[INFO] 10.244.0.166:49475 - 5759 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.009080648s
[INFO] 10.244.0.166:53592 - 6937 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.0091951s
[INFO] 10.244.0.166:34435 - 7338 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000163504s
[INFO] 10.244.0.166:50861 - 43598 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000126301s
[INFO] 10.244.0.166:52910 - 59188 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000152403s
[INFO] 10.244.0.166:37611 - 39075 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000133133s
[INFO] 10.244.0.166:52544 - 60633 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000117314s
[INFO] 10.244.0.166:43547 - 58064 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000162744s
[INFO] 10.244.0.166:54919 - 3653 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.005894846s
[INFO] 10.244.0.166:51820 - 14105 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.011708698s
[INFO] 10.244.0.166:42104 - 28143 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000215351s
[INFO] 10.244.0.166:37786 - 22493 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000111729s
[INFO] 10.244.0.166:58896 - 18540 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000162813s
[INFO] 10.244.0.166:45026 - 65025 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000102313s
[INFO] 10.244.0.166:38579 - 55024 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000120909s
[INFO] 10.244.0.166:40012 - 14932 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000131483s
[INFO] 10.244.0.166:50939 - 13460 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.010528227s
[INFO] 10.244.0.166:56916 - 15252 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.009417153s
[INFO] 10.244.0.166:60810 - 17725 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000198968s
[INFO] 10.244.0.166:38806 - 58950 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000282762s
[INFO] 10.244.0.166:33940 - 32765 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000170823s
[INFO] 10.244.0.166:56626 - 2445 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000126231s
[INFO] 10.244.0.166:52731 - 32201 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000118447s
[INFO] 10.244.0.166:32860 - 53257 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000131264s
[INFO] 10.244.0.166:46876 - 55139 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.010633466s
[INFO] 10.244.0.166:40038 - 51760 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.011410002s
[INFO] 10.244.0.166:35923 - 62153 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000182119s
[INFO] 10.244.0.166:48777 - 35058 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000111734s
[INFO] 10.244.0.166:56561 - 16460 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000249228s
[INFO] 10.244.0.166:57116 - 19050 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000145059s
[INFO] 10.244.0.166:37197 - 21923 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.00016332s
[INFO] 10.244.0.166:46994 - 52405 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000106749s
[INFO] 10.244.0.166:37918 - 40515 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.008026653s
[INFO] 10.244.0.166:59173 - 2670 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.008949094s
[INFO] 10.244.0.166:54845 - 50912 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000165624s
[INFO] 10.244.0.166:45462 - 57432 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000098834s
[INFO] 10.244.0.166:41315 - 29031 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000147918s
[INFO] 10.244.0.166:48888 - 744 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000127051s
[INFO] 10.244.0.166:51892 - 39798 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.00012058s
[INFO] 10.244.0.166:46867 - 28850 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000112617s
[INFO] 10.244.0.166:37393 - 24162 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.011407435s
[INFO] 10.244.0.166:36879 - 56453 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.011146092s
[INFO] 10.244.0.166:50736 - 39043 "AAAA IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000157436s
[INFO] 10.244.0.166:33954 - 35229 "A IN grafana.com.darwin.svc.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000094319s
[INFO] 10.244.0.166:38758 - 1743 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000138007s
[INFO] 10.244.0.166:48333 - 37648 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000107597s
[INFO] 10.244.0.166:48411 - 60874 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000136609s
[INFO] 10.244.0.166:41671 - 35005 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000199479s
[INFO] 10.244.0.166:47759 - 54357 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.009956632s
[INFO] 10.244.0.166:56720 - 48281 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.010302894s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_03_21T15_44_26_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 21 Mar 2024 10:14:22 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 19 May 2024 09:20:50 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 19 May 2024 09:17:46 +0000   Thu, 21 Mar 2024 10:14:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 19 May 2024 09:17:46 +0000   Thu, 21 Mar 2024 10:14:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 19 May 2024 09:17:46 +0000   Thu, 21 Mar 2024 10:14:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 19 May 2024 09:17:46 +0000   Thu, 21 Mar 2024 10:14:22 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  244506940Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16206916Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  244506940Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16206916Ki
  pods:               110
System Info:
  Machine ID:                 271339dc09ae4e36aa1fddb89ba10929
  System UUID:                fa1c29d1-0f68-46b3-b00e-946d4e81e90f
  Boot ID:                    6e719285-950d-4518-bb0b-6c1c9880a1d9
  Kernel Version:             6.5.0-28-generic
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (16 in total)
  Namespace                   Name                                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                      ------------  ----------  ---------------  -------------  ---
  argo-rollouts               argo-rollouts-5fc7cd6598-qqp4w                            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         35h
  darwin                      alertmanager-prometheus-kube-prometheus-alertmanager-0    0 (0%!)(MISSING)        0 (0%!)(MISSING)      200Mi (1%!)(MISSING)       0 (0%!)(MISSING)         16d
  darwin                      prometheus-grafana-86b6d8f896-jmzxj                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16d
  darwin                      prometheus-kube-prometheus-operator-54577f779b-4txmz      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16d
  darwin                      prometheus-kube-state-metrics-c8f945cbb-g7lm8             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16d
  darwin                      prometheus-prometheus-kube-prometheus-prometheus-0        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16d
  darwin                      prometheus-prometheus-node-exporter-sctkf                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16d
  default                     argo-rollouts-dashboard-5577ff8545-tr74x                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         97s
  kube-system                 coredns-5dd5756b68-rjtb2                                  100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     58d
  kube-system                 etcd-minikube                                             100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         58d
  kube-system                 kube-apiserver-minikube                                   250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         58d
  kube-system                 kube-controller-manager-minikube                          200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         58d
  kube-system                 kube-proxy-42gnw                                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         58d
  kube-system                 kube-scheduler-minikube                                   100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         58d
  kube-system                 metrics-server-7c66d45ddc-nlf8d                           100m (2%!)(MISSING)     0 (0%!)(MISSING)      200Mi (1%!)(MISSING)       0 (0%!)(MISSING)         13d
  kube-system                 storage-provisioner                                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         58d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%!)(MISSING)  0 (0%!)(MISSING)
  memory             570Mi (3%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.000003] debugfs: File 'radeon_ring_cp1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_cp2' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma1' in directory '0' already present!
[  +0.000002] debugfs: File 'radeon_ring_dma2' in directory '0' already present!
[  +1.351498] done.
[  +0.000089] thermal thermal_zone1: failed to read out thermal zone (-61)
[May19 07:09] Bluetooth: hci0: Opcode 0x0401 failed: -16
[  +5.996002] Bluetooth: hci0: Opcode 0x0401 failed: -16
[  +6.883957] Bluetooth: hci0: Opcode 0x0401 failed: -16
[May19 07:10] Bluetooth: hci0: Opcode 0x0401 failed: -16
[May19 07:12] Bluetooth: hci0: Opcode 0x0401 failed: -16
[May19 07:29] debugfs: File 'radeon_ring_gfx' in directory '0' already present!
[  +0.000002] debugfs: File 'radeon_ring_cp1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_cp2' in directory '0' already present!
[  +0.000002] debugfs: File 'radeon_ring_dma1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma2' in directory '0' already present!
[  +1.667350] debugfs: File 'radeon_ring_gfx' in directory '0' already present!
[  +0.000003] debugfs: File 'radeon_ring_cp1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_cp2' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma2' in directory '0' already present!
[  +0.723311] done.
[  +0.000032] thermal thermal_zone1: failed to read out thermal zone (-61)
[May19 07:49] debugfs: File 'radeon_ring_gfx' in directory '0' already present!
[  +0.000003] debugfs: File 'radeon_ring_cp1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_cp2' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma2' in directory '0' already present!
[  +1.669599] debugfs: File 'radeon_ring_gfx' in directory '0' already present!
[  +0.000003] debugfs: File 'radeon_ring_cp1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_cp2' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma2' in directory '0' already present!
[  +0.719268] done.
[  +0.000030] thermal thermal_zone1: failed to read out thermal zone (-61)
[May19 08:09] debugfs: File 'radeon_ring_gfx' in directory '0' already present!
[  +0.000002] debugfs: File 'radeon_ring_cp1' in directory '0' already present!
[  +0.000002] debugfs: File 'radeon_ring_cp2' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma2' in directory '0' already present!
[  +1.668204] debugfs: File 'radeon_ring_gfx' in directory '0' already present!
[  +0.000003] debugfs: File 'radeon_ring_cp1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_cp2' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma2' in directory '0' already present!
[  +0.710818] done.
[  +0.000028] thermal thermal_zone1: failed to read out thermal zone (-61)
[May19 08:31] debugfs: File 'radeon_ring_gfx' in directory '0' already present!
[  +0.000002] debugfs: File 'radeon_ring_cp1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_cp2' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma1' in directory '0' already present!
[  +0.000002] debugfs: File 'radeon_ring_dma2' in directory '0' already present!
[  +1.668661] debugfs: File 'radeon_ring_gfx' in directory '0' already present!
[  +0.000002] debugfs: File 'radeon_ring_cp1' in directory '0' already present!
[  +0.000002] debugfs: File 'radeon_ring_cp2' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma1' in directory '0' already present!
[  +0.000001] debugfs: File 'radeon_ring_dma2' in directory '0' already present!
[  +0.699811] done.
[  +0.000032] thermal thermal_zone1: failed to read out thermal zone (-61)
[May19 08:33] workqueue: delayed_fput hogged CPU for >10000us 128 times, consider switching to WQ_UNBOUND

* 
* ==> etcd [31a1fe5caa19] <==
* {"level":"info","ts":"2024-05-06T16:22:11.614602Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":848493}
{"level":"info","ts":"2024-05-06T16:22:11.620962Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":848493,"took":"5.495505ms","hash":3820236659}
{"level":"info","ts":"2024-05-06T16:22:11.621082Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3820236659,"revision":848493,"compact-revision":848102}
{"level":"info","ts":"2024-05-06T16:27:11.631818Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":848882}
{"level":"info","ts":"2024-05-06T16:27:11.636602Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":848882,"took":"3.690598ms","hash":22430749}
{"level":"info","ts":"2024-05-06T16:27:11.636679Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":22430749,"revision":848882,"compact-revision":848493}
{"level":"info","ts":"2024-05-06T16:32:11.644265Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":849275}
{"level":"info","ts":"2024-05-06T16:32:11.646273Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":849275,"took":"1.561337ms","hash":2232162780}
{"level":"info","ts":"2024-05-06T16:32:11.646302Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2232162780,"revision":849275,"compact-revision":848882}
{"level":"info","ts":"2024-05-06T16:37:11.656065Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":849664}
{"level":"info","ts":"2024-05-06T16:37:11.65795Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":849664,"took":"1.633235ms","hash":582288285}
{"level":"info","ts":"2024-05-06T16:37:11.657988Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":582288285,"revision":849664,"compact-revision":849275}
{"level":"info","ts":"2024-05-07T14:10:44.681377Z","caller":"traceutil/trace.go:171","msg":"trace[627789628] linearizableReadLoop","detail":"{readStateIndex:1035584; appliedIndex:1035583; }","duration":"341.123277ms","start":"2024-05-07T14:10:44.340237Z","end":"2024-05-07T14:10:44.68136Z","steps":["trace[627789628] 'read index received'  (duration: 341.000973ms)","trace[627789628] 'applied index is now lower than readState.Index'  (duration: 121.4¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-05-07T14:10:44.681468Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-07T14:10:44.328634Z","time spent":"352.831672ms","remote":"127.0.0.1:54004","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-05-07T14:10:44.681695Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"341.46731ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-07T14:10:44.681735Z","caller":"traceutil/trace.go:171","msg":"trace[1528971924] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:850260; }","duration":"341.513434ms","start":"2024-05-07T14:10:44.34021Z","end":"2024-05-07T14:10:44.681723Z","steps":["trace[1528971924] 'agreement among raft nodes before linearized reading'  (duration: 341.442668ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-07T14:10:44.681772Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-07T14:10:44.340198Z","time spent":"341.565227ms","remote":"127.0.0.1:53958","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-05-07T14:10:44.683292Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"341.965847ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2024-05-07T14:10:44.683346Z","caller":"traceutil/trace.go:171","msg":"trace[976519666] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:850260; }","duration":"342.025496ms","start":"2024-05-07T14:10:44.341305Z","end":"2024-05-07T14:10:44.683331Z","steps":["trace[976519666] 'agreement among raft nodes before linearized reading'  (duration: 341.918809ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-07T14:10:44.683377Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-07T14:10:44.341292Z","time spent":"342.077223ms","remote":"127.0.0.1:54164","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1136,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2024-05-07T14:10:44.683837Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"251.487189ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" ","response":"range_response_count:12 size:430834"}
{"level":"info","ts":"2024-05-07T14:10:44.683883Z","caller":"traceutil/trace.go:171","msg":"trace[1735377250] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:12; response_revision:850260; }","duration":"251.542655ms","start":"2024-05-07T14:10:44.43233Z","end":"2024-05-07T14:10:44.683872Z","steps":["trace[1735377250] 'agreement among raft nodes before linearized reading'  (duration: 251.312733ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-07T14:10:44.686691Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"276.978001ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/default/default\" ","response":"range_response_count:1 size:172"}
{"level":"info","ts":"2024-05-07T14:10:44.686742Z","caller":"traceutil/trace.go:171","msg":"trace[1208177711] range","detail":"{range_begin:/registry/serviceaccounts/default/default; range_end:; response_count:1; response_revision:850260; }","duration":"277.035649ms","start":"2024-05-07T14:10:44.409694Z","end":"2024-05-07T14:10:44.68673Z","steps":["trace[1208177711] 'agreement among raft nodes before linearized reading'  (duration: 276.937965ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-07T14:10:44.686922Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"277.255291ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/default/default\" ","response":"range_response_count:1 size:172"}
{"level":"info","ts":"2024-05-07T14:10:44.686949Z","caller":"traceutil/trace.go:171","msg":"trace[1371275286] range","detail":"{range_begin:/registry/serviceaccounts/default/default; range_end:; response_count:1; response_revision:850260; }","duration":"277.286284ms","start":"2024-05-07T14:10:44.409655Z","end":"2024-05-07T14:10:44.686941Z","steps":["trace[1371275286] 'agreement among raft nodes before linearized reading'  (duration: 277.228182ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-07T14:10:44.68823Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"305.772014ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/argo-rollouts/argo-rollouts-controller-lock\" ","response":"range_response_count:1 size:544"}
{"level":"info","ts":"2024-05-07T14:10:44.688281Z","caller":"traceutil/trace.go:171","msg":"trace[831100117] range","detail":"{range_begin:/registry/leases/argo-rollouts/argo-rollouts-controller-lock; range_end:; response_count:1; response_revision:850260; }","duration":"305.832917ms","start":"2024-05-07T14:10:44.382436Z","end":"2024-05-07T14:10:44.688269Z","steps":["trace[831100117] 'agreement among raft nodes before linearized reading'  (duration: 305.731454ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-07T14:10:44.688318Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-07T14:10:44.382423Z","time spent":"305.8853ms","remote":"127.0.0.1:54260","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":1,"response size":568,"request content":"key:\"/registry/leases/argo-rollouts/argo-rollouts-controller-lock\" "}
{"level":"warn","ts":"2024-05-07T14:10:44.688839Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"310.981548ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" ","response":"range_response_count:45 size:741253"}
{"level":"info","ts":"2024-05-07T14:10:44.688882Z","caller":"traceutil/trace.go:171","msg":"trace[1995627889] range","detail":"{range_begin:/registry/configmaps/; range_end:/registry/configmaps0; response_count:45; response_revision:850260; }","duration":"311.031999ms","start":"2024-05-07T14:10:44.377839Z","end":"2024-05-07T14:10:44.688871Z","steps":["trace[1995627889] 'agreement among raft nodes before linearized reading'  (duration: 310.608036ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-07T14:10:44.688916Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-07T14:10:44.377826Z","time spent":"311.079946ms","remote":"127.0.0.1:54098","response type":"/etcdserverpb.KV/Range","request count":0,"request size":46,"response count":45,"response size":741277,"request content":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" "}
{"level":"warn","ts":"2024-05-07T14:10:44.691734Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"316.328112ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/darwin/\" range_end:\"/registry/configmaps/darwin0\" ","response":"range_response_count:32 size:720815"}
{"level":"info","ts":"2024-05-07T14:10:44.691854Z","caller":"traceutil/trace.go:171","msg":"trace[744055980] range","detail":"{range_begin:/registry/configmaps/darwin/; range_end:/registry/configmaps/darwin0; response_count:32; response_revision:850260; }","duration":"316.457511ms","start":"2024-05-07T14:10:44.37537Z","end":"2024-05-07T14:10:44.691827Z","steps":["trace[744055980] 'agreement among raft nodes before linearized reading'  (duration: 315.951165ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-07T14:10:44.691899Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-07T14:10:44.375358Z","time spent":"316.526986ms","remote":"127.0.0.1:54098","response type":"/etcdserverpb.KV/Range","request count":0,"request size":60,"response count":32,"response size":720839,"request content":"key:\"/registry/configmaps/darwin/\" range_end:\"/registry/configmaps/darwin0\" "}
{"level":"warn","ts":"2024-05-07T14:10:44.694878Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"325.706982ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/darwin/\" range_end:\"/registry/secrets/darwin0\" ","response":"range_response_count:10 size:428412"}
{"level":"info","ts":"2024-05-07T14:10:44.694938Z","caller":"traceutil/trace.go:171","msg":"trace[378738180] range","detail":"{range_begin:/registry/secrets/darwin/; range_end:/registry/secrets/darwin0; response_count:10; response_revision:850260; }","duration":"325.777405ms","start":"2024-05-07T14:10:44.369146Z","end":"2024-05-07T14:10:44.694923Z","steps":["trace[378738180] 'agreement among raft nodes before linearized reading'  (duration: 325.305491ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-07T14:10:44.694979Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-07T14:10:44.369133Z","time spent":"325.833887ms","remote":"127.0.0.1:54082","response type":"/etcdserverpb.KV/Range","request count":0,"request size":54,"response count":10,"response size":428436,"request content":"key:\"/registry/secrets/darwin/\" range_end:\"/registry/secrets/darwin0\" "}
{"level":"info","ts":"2024-05-07T14:13:04.948092Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":850056}
{"level":"info","ts":"2024-05-07T14:13:04.952378Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":850056,"took":"3.31413ms","hash":3014455631}
{"level":"info","ts":"2024-05-07T14:13:04.952449Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3014455631,"revision":850056,"compact-revision":849664}
{"level":"info","ts":"2024-05-07T14:18:04.963242Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":850450}
{"level":"info","ts":"2024-05-07T14:18:04.967598Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":850450,"took":"3.3267ms","hash":3249079909}
{"level":"info","ts":"2024-05-07T14:18:04.96768Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3249079909,"revision":850450,"compact-revision":850056}
{"level":"info","ts":"2024-05-07T14:23:04.979741Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":850840}
{"level":"info","ts":"2024-05-07T14:23:04.984435Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":850840,"took":"3.555227ms","hash":1945137424}
{"level":"info","ts":"2024-05-07T14:23:04.984516Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1945137424,"revision":850840,"compact-revision":850450}
{"level":"info","ts":"2024-05-07T14:23:35.079979Z","caller":"traceutil/trace.go:171","msg":"trace[570803624] transaction","detail":"{read_only:false; response_revision:851269; number_of_response:1; }","duration":"101.938557ms","start":"2024-05-07T14:23:34.977991Z","end":"2024-05-07T14:23:35.07993Z","steps":["trace[570803624] 'process raft request'  (duration: 101.610551ms)"],"step_count":1}
{"level":"info","ts":"2024-05-07T14:26:05.715207Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-05-07T14:26:05.715325Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-05-07T14:26:05.71555Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-07T14:26:05.715794Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-07T14:26:05.785891Z","caller":"v3rpc/watch.go:500","msg":"failed to send watch control response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"warn","ts":"2024-05-07T14:26:06.085812Z","caller":"v3rpc/watch.go:473","msg":"failed to send watch response to gRPC stream","error":"rpc error: code = Unavailable desc = transport is closing"}
{"level":"warn","ts":"2024-05-07T14:26:06.910504Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-07T14:26:06.910598Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-05-07T14:26:06.910838Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-05-07T14:26:06.93781Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-07T14:26:06.938261Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-07T14:26:06.938334Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [394ca45e182d] <==
* {"level":"info","ts":"2024-05-19T08:22:48.059484Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1329951984,"revision":868334,"compact-revision":867944}
{"level":"info","ts":"2024-05-19T08:27:48.100134Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":868720}
{"level":"info","ts":"2024-05-19T08:27:48.102221Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":868720,"took":"1.661111ms","hash":106704064}
{"level":"info","ts":"2024-05-19T08:27:48.102272Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":106704064,"revision":868720,"compact-revision":868334}
{"level":"info","ts":"2024-05-19T08:31:05.655015Z","caller":"traceutil/trace.go:171","msg":"trace[1363608671] linearizableReadLoop","detail":"{readStateIndex:1057562; appliedIndex:1057561; }","duration":"323.164292ms","start":"2024-05-19T08:31:05.331832Z","end":"2024-05-19T08:31:05.654996Z","steps":["trace[1363608671] 'read index received'  (duration: 322.831187ms)","trace[1363608671] 'applied index is now lower than readState.Index'  (duration: 329.284¬µs)"],"step_count":2}
{"level":"info","ts":"2024-05-19T08:31:05.655544Z","caller":"traceutil/trace.go:171","msg":"trace[12702484] transaction","detail":"{read_only:false; response_revision:869255; number_of_response:1; }","duration":"325.843713ms","start":"2024-05-19T08:31:05.32968Z","end":"2024-05-19T08:31:05.655524Z","steps":["trace[12702484] 'process raft request'  (duration: 325.022403ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-19T08:31:05.65565Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-19T08:31:05.329667Z","time spent":"325.920403ms","remote":"127.0.0.1:34242","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:869242 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-05-19T08:31:05.655936Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"324.118234ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:136"}
{"level":"info","ts":"2024-05-19T08:31:05.655972Z","caller":"traceutil/trace.go:171","msg":"trace[1042572370] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:869255; }","duration":"324.159846ms","start":"2024-05-19T08:31:05.331802Z","end":"2024-05-19T08:31:05.655962Z","steps":["trace[1042572370] 'agreement among raft nodes before linearized reading'  (duration: 324.068444ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-19T08:31:05.656005Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-19T08:31:05.331788Z","time spent":"324.207245ms","remote":"127.0.0.1:34028","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":160,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"warn","ts":"2024-05-19T08:31:05.656185Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"302.87107ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-19T08:31:05.65623Z","caller":"traceutil/trace.go:171","msg":"trace[1433598845] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:869255; }","duration":"302.915274ms","start":"2024-05-19T08:31:05.353305Z","end":"2024-05-19T08:31:05.65622Z","steps":["trace[1433598845] 'agreement among raft nodes before linearized reading'  (duration: 302.852884ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-19T08:31:05.656259Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-19T08:31:05.353292Z","time spent":"302.958996ms","remote":"127.0.0.1:33978","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-05-19T08:31:05.656515Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"160.049106ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2024-05-19T08:31:05.656554Z","caller":"traceutil/trace.go:171","msg":"trace[1862653882] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:869255; }","duration":"160.089739ms","start":"2024-05-19T08:31:05.496453Z","end":"2024-05-19T08:31:05.656543Z","steps":["trace[1862653882] 'agreement among raft nodes before linearized reading'  (duration: 160.007066ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-19T08:31:05.656743Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"255.424452ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/argo-rollouts/argo-rollouts-controller-lock\" ","response":"range_response_count:1 size:544"}
{"level":"info","ts":"2024-05-19T08:31:05.656772Z","caller":"traceutil/trace.go:171","msg":"trace[2039881802] range","detail":"{range_begin:/registry/leases/argo-rollouts/argo-rollouts-controller-lock; range_end:; response_count:1; response_revision:869255; }","duration":"255.455016ms","start":"2024-05-19T08:31:05.401308Z","end":"2024-05-19T08:31:05.656763Z","steps":["trace[2039881802] 'agreement among raft nodes before linearized reading'  (duration: 255.3985ms)"],"step_count":1}
{"level":"info","ts":"2024-05-19T08:34:09.040673Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":869109}
{"level":"info","ts":"2024-05-19T08:34:09.042799Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":869109,"took":"1.656563ms","hash":1963793184}
{"level":"info","ts":"2024-05-19T08:34:09.043042Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1963793184,"revision":869109,"compact-revision":868720}
{"level":"info","ts":"2024-05-19T08:39:09.071344Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":869493}
{"level":"info","ts":"2024-05-19T08:39:09.07342Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":869493,"took":"1.798168ms","hash":2170443504}
{"level":"info","ts":"2024-05-19T08:39:09.073455Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2170443504,"revision":869493,"compact-revision":869109}
{"level":"info","ts":"2024-05-19T08:39:20.847412Z","caller":"traceutil/trace.go:171","msg":"trace[838116017] linearizableReadLoop","detail":"{readStateIndex:1058314; appliedIndex:1058313; }","duration":"193.506566ms","start":"2024-05-19T08:39:20.653855Z","end":"2024-05-19T08:39:20.847361Z","steps":["trace[838116017] 'read index received'  (duration: 193.38518ms)","trace[838116017] 'applied index is now lower than readState.Index'  (duration: 117.375¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-05-19T08:39:20.847659Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"193.848228ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/argo-rollouts/argo-rollouts-controller-lock\" ","response":"range_response_count:1 size:545"}
{"level":"info","ts":"2024-05-19T08:39:20.847854Z","caller":"traceutil/trace.go:171","msg":"trace[365092984] range","detail":"{range_begin:/registry/leases/argo-rollouts/argo-rollouts-controller-lock; range_end:; response_count:1; response_revision:869904; }","duration":"194.051472ms","start":"2024-05-19T08:39:20.65375Z","end":"2024-05-19T08:39:20.847802Z","steps":["trace[365092984] 'agreement among raft nodes before linearized reading'  (duration: 193.735793ms)"],"step_count":1}
{"level":"info","ts":"2024-05-19T08:39:20.97699Z","caller":"traceutil/trace.go:171","msg":"trace[1655331041] transaction","detail":"{read_only:false; response_revision:869905; number_of_response:1; }","duration":"118.989709ms","start":"2024-05-19T08:39:20.857983Z","end":"2024-05-19T08:39:20.976972Z","steps":["trace[1655331041] 'process raft request'  (duration: 114.116515ms)"],"step_count":1}
{"level":"info","ts":"2024-05-19T08:39:21.123412Z","caller":"traceutil/trace.go:171","msg":"trace[377149134] transaction","detail":"{read_only:false; response_revision:869906; number_of_response:1; }","duration":"107.76975ms","start":"2024-05-19T08:39:21.015585Z","end":"2024-05-19T08:39:21.123355Z","steps":["trace[377149134] 'process raft request'  (duration: 99.204926ms)"],"step_count":1}
{"level":"info","ts":"2024-05-19T08:39:27.389615Z","caller":"traceutil/trace.go:171","msg":"trace[674477113] transaction","detail":"{read_only:false; response_revision:869915; number_of_response:1; }","duration":"106.455998ms","start":"2024-05-19T08:39:27.28314Z","end":"2024-05-19T08:39:27.389596Z","steps":["trace[674477113] 'process raft request'  (duration: 106.303179ms)"],"step_count":1}
{"level":"info","ts":"2024-05-19T08:39:29.162153Z","caller":"traceutil/trace.go:171","msg":"trace[1847778960] transaction","detail":"{read_only:false; response_revision:869916; number_of_response:1; }","duration":"105.488456ms","start":"2024-05-19T08:39:29.05662Z","end":"2024-05-19T08:39:29.162109Z","steps":["trace[1847778960] 'process raft request'  (duration: 105.212852ms)"],"step_count":1}
{"level":"info","ts":"2024-05-19T08:39:29.500948Z","caller":"traceutil/trace.go:171","msg":"trace[567565971] transaction","detail":"{read_only:false; response_revision:869917; number_of_response:1; }","duration":"105.565267ms","start":"2024-05-19T08:39:29.395364Z","end":"2024-05-19T08:39:29.500929Z","steps":["trace[567565971] 'process raft request'  (duration: 105.449879ms)"],"step_count":1}
{"level":"info","ts":"2024-05-19T08:44:11.131633Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":869891}
{"level":"info","ts":"2024-05-19T08:44:11.133206Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":869891,"took":"1.272749ms","hash":1125653909}
{"level":"info","ts":"2024-05-19T08:44:11.133239Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1125653909,"revision":869891,"compact-revision":869493}
{"level":"info","ts":"2024-05-19T08:49:11.145155Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":870278}
{"level":"info","ts":"2024-05-19T08:49:11.147204Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":870278,"took":"1.818781ms","hash":1960206537}
{"level":"info","ts":"2024-05-19T08:49:11.147243Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1960206537,"revision":870278,"compact-revision":869891}
{"level":"info","ts":"2024-05-19T08:54:11.162212Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":870665}
{"level":"info","ts":"2024-05-19T08:54:11.163726Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":870665,"took":"1.24411ms","hash":1778005494}
{"level":"info","ts":"2024-05-19T08:54:11.163754Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1778005494,"revision":870665,"compact-revision":870278}
{"level":"info","ts":"2024-05-19T08:59:10.237288Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":1060106,"local-member-snapshot-index":1050105,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-05-19T08:59:10.253161Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":1060106}
{"level":"info","ts":"2024-05-19T08:59:10.253372Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":1055106}
{"level":"info","ts":"2024-05-19T08:59:10.493196Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000008-00000000000f69b5.snap"}
{"level":"info","ts":"2024-05-19T08:59:11.173264Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":871053}
{"level":"info","ts":"2024-05-19T08:59:11.174735Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":871053,"took":"1.24461ms","hash":4083322727}
{"level":"info","ts":"2024-05-19T08:59:11.174763Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4083322727,"revision":871053,"compact-revision":870665}
{"level":"info","ts":"2024-05-19T09:04:11.189082Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":871458}
{"level":"info","ts":"2024-05-19T09:04:11.194875Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":871458,"took":"4.643054ms","hash":609513562}
{"level":"info","ts":"2024-05-19T09:04:11.19498Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":609513562,"revision":871458,"compact-revision":871053}
{"level":"info","ts":"2024-05-19T09:09:11.205981Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":871869}
{"level":"info","ts":"2024-05-19T09:09:11.210798Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":871869,"took":"4.035525ms","hash":3785093958}
{"level":"info","ts":"2024-05-19T09:09:11.210875Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3785093958,"revision":871869,"compact-revision":871458}
{"level":"info","ts":"2024-05-19T09:11:09.85038Z","caller":"traceutil/trace.go:171","msg":"trace[817571119] transaction","detail":"{read_only:false; response_revision:872411; number_of_response:1; }","duration":"106.758208ms","start":"2024-05-19T09:11:09.743588Z","end":"2024-05-19T09:11:09.850346Z","steps":["trace[817571119] 'process raft request'  (duration: 106.537946ms)"],"step_count":1}
{"level":"info","ts":"2024-05-19T09:14:11.230573Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":872257}
{"level":"info","ts":"2024-05-19T09:14:11.234262Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":872257,"took":"3.002585ms","hash":2222756862}
{"level":"info","ts":"2024-05-19T09:14:11.234328Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2222756862,"revision":872257,"compact-revision":871869}
{"level":"info","ts":"2024-05-19T09:19:11.246451Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":872644}
{"level":"info","ts":"2024-05-19T09:19:11.251206Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":872644,"took":"3.991992ms","hash":3802724047}
{"level":"info","ts":"2024-05-19T09:19:11.251288Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3802724047,"revision":872644,"compact-revision":872257}

* 
* ==> kernel <==
*  09:20:56 up 5 days, 13:22,  0 users,  load average: 1.44, 2.64, 2.55
Linux minikube 6.5.0-28-generic #29~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr  4 14:39:20 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [5627bfd0b427] <==
* I0519 09:09:10.770726       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:09:10.771720       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:09:10.776725       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:09:10.777665       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:09:10.784693       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:09:10.792537       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:09:10.805948       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0519 09:09:10.806236       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:09:10.810296       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:09:10.812255       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0519 09:09:10.829237       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:09:10.830001       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:09:14.415174       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:10:10.504522       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:11:10.508892       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:12:10.517663       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:13:10.504715       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:14:10.508657       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:14:10.762224       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:14:10.762560       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:14:10.767261       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:14:10.767573       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:14:10.768131       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:14:10.778115       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:14:10.778495       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:14:10.778927       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:14:10.782633       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0519 09:14:10.783617       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:14:10.799370       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:14:10.814817       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:14:10.816427       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:14:10.831443       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:14:10.835748       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:14:10.836123       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0519 09:14:10.843573       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0519 09:14:14.426311       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:15:10.507492       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:16:10.508078       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:17:10.505658       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:18:10.507870       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:19:10.499092       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:19:10.762665       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:19:10.763798       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:19:10.764660       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:19:10.765642       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:19:10.766558       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:19:10.768580       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:19:10.780375       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:19:10.780928       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:19:10.783749       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0519 09:19:10.785414       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:19:10.786626       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:19:10.800356       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0519 09:19:10.815547       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:19:10.818953       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:19:10.836131       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0519 09:19:10.836472       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0519 09:19:14.431940       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0519 09:19:25.104048       1 alloc.go:330] "allocated clusterIPs" service="default/argo-rollouts-server" clusterIPs={"IPv4":"10.96.83.34"}
I0519 09:20:10.500832       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager

* 
* ==> kube-apiserver [d0b0f9408e2f] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 14:26:07.800283       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 14:26:07.800404       1 logging.go:59] [core] [Channel #193 SubChannel #194] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 14:26:07.800549       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 14:26:07.800635       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 14:26:07.800750       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 14:26:07.801158       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 14:26:07.813047       1 logging.go:59] [core] [Channel #202 SubChannel #203] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [622ab4da3827] <==
* I0505 11:10:16.648050       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/webapp-rollout-559576467" duration="89.803221ms"
I0505 11:10:16.648147       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/webapp-rollout-559576467" duration="53.666¬µs"
I0505 11:10:17.319953       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/webapp-rollout-559576467" duration="154.281¬µs"
I0505 11:10:17.399427       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/webapp-rollout-559576467" duration="81.98¬µs"
I0505 11:10:17.666043       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/webapp-rollout-559576467" duration="66.896¬µs"
I0505 11:10:17.671352       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/webapp-rollout-559576467" duration="73.063¬µs"
I0505 11:11:27.160682       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/webapp-rollout-689cdbccb9" duration="8.742¬µs"
I0505 11:11:27.170986       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/webapp-rollout-559576467" duration="15.112¬µs"
I0505 11:11:27.202867       1 job_controller.go:562] "enqueueing job" key="default/d420c7cb-38bb-42a8-b564-d1066f2b6f29.success-rate.1"
I0505 16:27:22.772836       1 event.go:307] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-7c66d45ddc to 1"
I0505 16:27:22.799834       1 event.go:307] "Event occurred" object="kube-system/metrics-server-7c66d45ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-7c66d45ddc-nlf8d"
I0505 16:27:22.824497       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="52.519524ms"
I0505 16:27:22.847238       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="22.626349ms"
I0505 16:27:22.847407       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="55.682¬µs"
I0505 16:27:22.854032       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="160.91¬µs"
I0505 16:27:34.169028       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="74.398¬µs"
E0505 16:27:52.111897       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0505 16:27:52.177120       1 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E0505 16:28:22.136823       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I0505 16:28:22.214099       1 garbagecollector.go:816] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I0505 16:28:43.224061       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="27.237823ms"
I0505 16:28:43.226435       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="202.176¬µs"
I0505 16:48:12.749116       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="18.737274ms"
I0505 16:48:12.749468       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="50.985¬µs"
I0505 16:48:12.807378       1 event.go:307] "Event occurred" object="default/rollouts-demo-75f7cbdf5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: rollouts-demo-75f7cbdf5-nmldv"
I0505 16:48:12.826420       1 event.go:307] "Event occurred" object="default/rollouts-demo-75f7cbdf5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: rollouts-demo-75f7cbdf5-s7mdf"
I0505 16:48:12.827289       1 event.go:307] "Event occurred" object="default/rollouts-demo-75f7cbdf5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: rollouts-demo-75f7cbdf5-2mw6f"
I0505 16:48:12.854050       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="60.65026ms"
I0505 16:48:12.878155       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="24.014009ms"
I0505 16:48:12.896622       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="18.413186ms"
I0505 16:48:12.896720       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="59.321¬µs"
I0505 16:48:12.903006       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="85.779¬µs"
I0505 16:48:12.919693       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="51.538¬µs"
I0505 16:48:15.290291       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="11.416828ms"
I0505 16:48:15.290394       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="57.132¬µs"
I0505 16:48:15.322384       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="9.039534ms"
I0505 16:48:15.322834       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="83.979¬µs"
I0505 16:48:15.351669       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="15.880405ms"
I0505 16:48:15.351820       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/rollouts-demo-75f7cbdf5" duration="117.426¬µs"
I0505 16:48:27.543509       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
E0505 16:48:27.570772       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Rollout/default/rollouts-demo: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
I0505 16:48:27.571266       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I0505 16:48:42.614914       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0505 16:48:42.630291       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Rollout/default/rollouts-demo: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0505 16:48:42.630624       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0505 16:48:57.659383       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Rollout/default/rollouts-demo: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0505 16:48:57.659784       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0505 16:48:57.659892       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0505 16:49:12.719586       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Rollout/default/rollouts-demo: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0505 16:49:12.721144       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0505 16:49:12.721644       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0505 16:49:27.739409       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Rollout/default/rollouts-demo: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0505 16:49:27.739987       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0505 16:49:27.740049       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0506 15:24:27.596398       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="Unauthorized"
E0506 15:24:27.897885       1 horizontal.go:274] failed to query scale subresource for Rollout/default/rollouts-demo: Unauthorized
I0507 14:10:45.626031       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="Unauthorized"
E0507 14:10:45.708242       1 horizontal.go:274] failed to query scale subresource for Rollout/default/rollouts-demo: Unauthorized
E0507 14:11:09.554698       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0507 14:11:09.554808       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"

* 
* ==> kube-controller-manager [fc1c22bdd914] <==
* I0518 07:04:03.406597       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="rollouts.argoproj.io \"rollouts-demo\" not found"
E0518 07:04:18.415023       1 horizontal.go:274] failed to query scale subresource for Rollout/default/rollouts-demo: rollouts.argoproj.io "rollouts-demo" not found
I0518 07:04:18.415279       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="rollouts.argoproj.io \"rollouts-demo\" not found"
E0518 07:04:33.444510       1 horizontal.go:274] failed to query scale subresource for Rollout/default/rollouts-demo: rollouts.argoproj.io "rollouts-demo" not found
I0518 07:04:33.444508       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="rollouts.argoproj.io \"rollouts-demo\" not found"
I0518 07:04:38.700553       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-764c86c5fc" duration="16.553607ms"
E0518 07:04:38.700633       1 replica_set.go:557] sync "default/argo-rollouts-dashboard-764c86c5fc" failed with pods "argo-rollouts-dashboard-764c86c5fc-" is forbidden: error looking up service account default/argo-rollouts-dashboard: serviceaccount "argo-rollouts-dashboard" not found
I0518 07:04:38.700572       1 event.go:307] "Event occurred" object="default/argo-rollouts-dashboard-764c86c5fc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"argo-rollouts-dashboard-764c86c5fc-\" is forbidden: error looking up service account default/argo-rollouts-dashboard: serviceaccount \"argo-rollouts-dashboard\" not found"
E0518 07:04:48.451106       1 horizontal.go:274] failed to query scale subresource for Rollout/default/rollouts-demo: rollouts.argoproj.io "rollouts-demo" not found
I0518 07:04:48.451172       1 event.go:307] "Event occurred" object="default/rollouts-demo-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="rollouts.argoproj.io \"rollouts-demo\" not found"
I0518 07:05:03.452208       1 horizontal.go:512] "Horizontal Pod Autoscaler has been deleted" HPA="default/rollouts-demo-hpa"
I0518 07:07:22.559297       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-764c86c5fc" duration="18.295963ms"
I0518 07:07:22.559364       1 event.go:307] "Event occurred" object="default/argo-rollouts-dashboard-764c86c5fc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"argo-rollouts-dashboard-764c86c5fc-\" is forbidden: error looking up service account default/argo-rollouts-dashboard: serviceaccount \"argo-rollouts-dashboard\" not found"
E0518 07:07:22.559411       1 replica_set.go:557] sync "default/argo-rollouts-dashboard-764c86c5fc" failed with pods "argo-rollouts-dashboard-764c86c5fc-" is forbidden: error looking up service account default/argo-rollouts-dashboard: serviceaccount "argo-rollouts-dashboard" not found
I0518 07:08:22.500608       1 event.go:307] "Event occurred" object="default/argo-rollouts-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set argo-rollouts-dashboard-5577ff8545 to 1"
I0518 07:08:22.533599       1 event.go:307] "Event occurred" object="default/argo-rollouts-dashboard-5577ff8545" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: argo-rollouts-dashboard-5577ff8545-7jwk5"
I0518 07:08:22.546310       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="45.610346ms"
I0518 07:08:22.591622       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="44.985761ms"
I0518 07:08:22.591913       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="141.145¬µs"
I0518 07:08:22.592259       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="197.39¬µs"
I0518 07:08:52.748040       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="9.950539ms"
I0518 07:08:52.748107       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="38.496¬µs"
I0518 07:08:52.755307       1 event.go:307] "Event occurred" object="default/argo-rollouts-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set argo-rollouts-dashboard-764c86c5fc to 0 from 1"
I0518 07:08:52.772104       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-764c86c5fc" duration="17.271973ms"
I0518 07:08:52.773470       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-764c86c5fc" duration="122.092¬µs"
I0518 07:10:47.965452       1 event.go:307] "Event occurred" object="argo-rollouts/argo-rollouts-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set argo-rollouts-dashboard-5577ff8545 to 1"
I0518 07:10:47.981983       1 event.go:307] "Event occurred" object="argo-rollouts/argo-rollouts-dashboard-5577ff8545" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: argo-rollouts-dashboard-5577ff8545-lqm97"
I0518 07:10:47.994596       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="29.988645ms"
I0518 07:10:48.017179       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="22.539311ms"
I0518 07:10:48.042806       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="25.581521ms"
I0518 07:10:48.043154       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="54.696¬µs"
I0518 07:10:54.041345       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="12.831678ms"
I0518 07:10:54.041495       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="69.294¬µs"
I0518 07:12:50.240128       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-764c86c5fc" duration="204.271¬µs"
E0518 10:34:13.664527       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0518 10:34:13.664574       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E0519 00:29:52.538978       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0519 00:29:52.539019       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
I0519 08:08:39.175172       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E0519 08:08:39.175484       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0519 08:34:54.603936       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-764c86c5fc" duration="8.57¬µs"
I0519 08:34:54.603945       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="5.507¬µs"
I0519 08:55:04.154404       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="6.589¬µs"
I0519 09:02:56.302710       1 event.go:307] "Event occurred" object="argo-rollouts/argo-rollouts-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set argo-rollouts-dashboard-5577ff8545 to 1"
I0519 09:02:56.332177       1 event.go:307] "Event occurred" object="argo-rollouts/argo-rollouts-dashboard-5577ff8545" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: argo-rollouts-dashboard-5577ff8545-xv4x7"
I0519 09:02:56.356406       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="53.770208ms"
I0519 09:02:56.407338       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="50.808186ms"
I0519 09:02:56.407501       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="90.368¬µs"
I0519 09:02:56.414438       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="117.286¬µs"
I0519 09:03:03.066771       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="17.078653ms"
I0519 09:03:03.068238       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="348.676¬µs"
I0519 09:18:54.998088       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argo-rollouts/argo-rollouts-dashboard-5577ff8545" duration="9.489¬µs"
I0519 09:19:19.532729       1 event.go:307] "Event occurred" object="default/argo-rollouts-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set argo-rollouts-dashboard-5577ff8545 to 1"
I0519 09:19:19.549167       1 event.go:307] "Event occurred" object="default/argo-rollouts-dashboard-5577ff8545" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: argo-rollouts-dashboard-5577ff8545-tr74x"
I0519 09:19:19.565697       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="33.168524ms"
I0519 09:19:19.578349       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="12.340367ms"
I0519 09:19:19.578879       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="81.642¬µs"
I0519 09:19:19.585106       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="93.685¬µs"
I0519 09:19:25.601509       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="11.705669ms"
I0519 09:19:25.601988       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/argo-rollouts-dashboard-5577ff8545" duration="187.351¬µs"

* 
* ==> kube-proxy [69ec33358294] <==
* I0505 09:43:04.008279       1 server_others.go:69] "Using iptables proxy"
I0505 09:43:04.324368       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0505 09:43:04.835575       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0505 09:43:04.880016       1 server_others.go:152] "Using iptables Proxier"
I0505 09:43:04.880073       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0505 09:43:04.880094       1 server_others.go:438] "Defaulting to no-op detect-local"
I0505 09:43:04.913769       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0505 09:43:04.914040       1 server.go:846] "Version info" version="v1.28.3"
I0505 09:43:04.914053       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0505 09:43:04.923542       1 config.go:188] "Starting service config controller"
I0505 09:43:04.923568       1 shared_informer.go:311] Waiting for caches to sync for service config
I0505 09:43:04.923595       1 config.go:97] "Starting endpoint slice config controller"
I0505 09:43:04.923601       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0505 09:43:04.925382       1 config.go:315] "Starting node config controller"
I0505 09:43:04.925406       1 shared_informer.go:311] Waiting for caches to sync for node config
I0505 09:43:05.024572       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0505 09:43:05.024644       1 shared_informer.go:318] Caches are synced for service config
I0505 09:43:05.025527       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [85d37d6fedd6] <==
* I0517 20:43:41.971498       1 server_others.go:69] "Using iptables proxy"
I0517 20:43:42.160378       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0517 20:43:42.582799       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0517 20:43:42.587788       1 server_others.go:152] "Using iptables Proxier"
I0517 20:43:42.587864       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0517 20:43:42.587884       1 server_others.go:438] "Defaulting to no-op detect-local"
I0517 20:43:42.587915       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0517 20:43:42.588234       1 server.go:846] "Version info" version="v1.28.3"
I0517 20:43:42.588258       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0517 20:43:42.618403       1 config.go:188] "Starting service config controller"
I0517 20:43:42.618432       1 shared_informer.go:311] Waiting for caches to sync for service config
I0517 20:43:42.618474       1 config.go:97] "Starting endpoint slice config controller"
I0517 20:43:42.618485       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0517 20:43:42.621137       1 config.go:315] "Starting node config controller"
I0517 20:43:42.621214       1 shared_informer.go:311] Waiting for caches to sync for node config
I0517 20:43:42.719265       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0517 20:43:42.719324       1 shared_informer.go:318] Caches are synced for service config
I0517 20:43:42.721561       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [1fc1c1e587b8] <==
* W0505 09:42:49.959934       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:49.960565       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:49.962159       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
I0505 09:42:49.959113       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0505 09:42:49.962869       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:49.963006       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:49.963306       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:49.963427       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:49.963744       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:49.963856       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:49.964102       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:49.964252       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:49.964546       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:49.964664       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:49.964687       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:49.964796       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:49.964797       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:49.964806       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:49.964903       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:49.965445       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:49.965547       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:50.796209       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:50.796344       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:50.823301       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0505 09:42:50.823338       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0505 09:42:55.608717       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0505 09:42:55.608831       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0505 09:42:55.609017       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0505 09:42:55.609084       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0505 09:42:55.609209       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0505 09:42:55.609285       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0505 09:42:55.609401       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0505 09:42:55.609453       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0505 09:42:55.609572       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0505 09:42:55.609624       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0505 09:42:55.609734       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0505 09:42:55.609788       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0505 09:42:55.609962       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0505 09:42:55.610015       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0505 09:42:55.610192       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0505 09:42:55.610246       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0505 09:42:55.610380       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0505 09:42:55.610436       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0505 09:42:55.610560       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0505 09:42:55.610614       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0505 09:42:55.610650       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0505 09:42:55.610711       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0505 09:42:55.617733       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0505 09:42:55.617766       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0505 09:42:55.618556       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0505 09:42:55.618692       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0505 09:42:55.618816       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0505 09:42:55.618857       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0505 09:42:55.618896       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0505 09:42:55.620631       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
I0505 09:42:58.649023       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0507 14:26:05.711249       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0507 14:26:05.713979       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0507 14:26:05.714811       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0507 14:26:05.719265       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [519dc8dbe10d] <==
* E0517 20:43:26.137016       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.137011       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.137365       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.136778       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.137579       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.136661       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.137615       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.136978       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.137829       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.138522       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.138563       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.138762       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.138839       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.138903       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.138976       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.138990       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.139057       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.139120       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.139178       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.139208       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.139307       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.139520       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.139400       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.139324       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.139613       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.139669       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.139635       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:26.139498       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0517 20:43:26.139719       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0517 20:43:31.257051       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0517 20:43:31.257667       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0517 20:43:31.257144       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0517 20:43:31.262574       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0517 20:43:31.257218       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0517 20:43:31.262742       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0517 20:43:31.257301       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0517 20:43:31.262914       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0517 20:43:31.257327       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0517 20:43:31.257363       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0517 20:43:31.257412       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0517 20:43:31.257411       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0517 20:43:31.257463       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0517 20:43:31.257490       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0517 20:43:31.257536       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0517 20:43:31.257555       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0517 20:43:31.257609       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0517 20:43:31.257629       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0517 20:43:31.263313       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0517 20:43:31.263341       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0517 20:43:31.263489       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0517 20:43:31.263546       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0517 20:43:31.263662       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0517 20:43:31.263725       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0517 20:43:31.263845       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0517 20:43:31.263889       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0517 20:43:31.263984       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0517 20:43:31.266614       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0517 20:43:31.283160       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0517 20:43:31.283197       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0517 20:43:33.105117       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* May 18 07:08:22 minikube kubelet[1570]: I0518 07:08:22.542766    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:08:22 minikube kubelet[1570]: I0518 07:08:22.542776    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:08:22 minikube kubelet[1570]: I0518 07:08:22.666255    1570 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hqt9s\" (UniqueName: \"kubernetes.io/projected/309fe6fb-d94c-47e6-bba8-badc5d6042a4-kube-api-access-hqt9s\") pod \"argo-rollouts-dashboard-5577ff8545-7jwk5\" (UID: \"309fe6fb-d94c-47e6-bba8-badc5d6042a4\") " pod="default/argo-rollouts-dashboard-5577ff8545-7jwk5"
May 18 07:08:52 minikube kubelet[1570]: I0518 07:08:52.739041    1570 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/argo-rollouts-dashboard-5577ff8545-7jwk5" podStartSLOduration=3.042718172 podCreationTimestamp="2024-05-18 07:08:22 +0000 UTC" firstStartedPulling="2024-05-18 07:08:23.236237872 +0000 UTC m=+5949.276837107" lastFinishedPulling="2024-05-18 07:08:50.932518737 +0000 UTC m=+5976.973118022" observedRunningTime="2024-05-18 07:08:52.73862062 +0000 UTC m=+5978.779219858" watchObservedRunningTime="2024-05-18 07:08:52.738999087 +0000 UTC m=+5978.779598317"
May 18 07:10:47 minikube kubelet[1570]: I0518 07:10:47.996387    1570 topology_manager.go:215] "Topology Admit Handler" podUID="7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2" podNamespace="argo-rollouts" podName="argo-rollouts-dashboard-5577ff8545-lqm97"
May 18 07:10:47 minikube kubelet[1570]: E0518 07:10:47.996484    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:10:47 minikube kubelet[1570]: E0518 07:10:47.996517    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:10:47 minikube kubelet[1570]: E0518 07:10:47.996532    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:10:47 minikube kubelet[1570]: E0518 07:10:47.996544    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:10:47 minikube kubelet[1570]: I0518 07:10:47.996615    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:10:47 minikube kubelet[1570]: I0518 07:10:47.996633    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:10:47 minikube kubelet[1570]: I0518 07:10:47.996644    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:10:47 minikube kubelet[1570]: I0518 07:10:47.996656    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:10:47 minikube kubelet[1570]: I0518 07:10:47.996676    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:10:47 minikube kubelet[1570]: I0518 07:10:47.996687    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 18 07:10:48 minikube kubelet[1570]: I0518 07:10:48.084859    1570 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-g7nr4\" (UniqueName: \"kubernetes.io/projected/7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2-kube-api-access-g7nr4\") pod \"argo-rollouts-dashboard-5577ff8545-lqm97\" (UID: \"7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2\") " pod="argo-rollouts/argo-rollouts-dashboard-5577ff8545-lqm97"
May 18 07:10:48 minikube kubelet[1570]: I0518 07:10:48.731162    1570 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="93f53a307b771db5b1430c1dde3807be8646681b2031c3c6d40725083011028f"
May 18 07:10:54 minikube kubelet[1570]: I0518 07:10:54.034421    1570 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="argo-rollouts/argo-rollouts-dashboard-5577ff8545-lqm97" podStartSLOduration=2.475904876 podCreationTimestamp="2024-05-18 07:10:47 +0000 UTC" firstStartedPulling="2024-05-18 07:10:48.771128313 +0000 UTC m=+6094.811727555" lastFinishedPulling="2024-05-18 07:10:53.329584719 +0000 UTC m=+6099.370183973" observedRunningTime="2024-05-18 07:10:54.034070053 +0000 UTC m=+6100.074669287" watchObservedRunningTime="2024-05-18 07:10:54.034361294 +0000 UTC m=+6100.074960531"
May 19 08:34:55 minikube kubelet[1570]: I0519 08:34:55.098999    1570 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-hqt9s\" (UniqueName: \"kubernetes.io/projected/309fe6fb-d94c-47e6-bba8-badc5d6042a4-kube-api-access-hqt9s\") pod \"309fe6fb-d94c-47e6-bba8-badc5d6042a4\" (UID: \"309fe6fb-d94c-47e6-bba8-badc5d6042a4\") "
May 19 08:34:55 minikube kubelet[1570]: I0519 08:34:55.108300    1570 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/309fe6fb-d94c-47e6-bba8-badc5d6042a4-kube-api-access-hqt9s" (OuterVolumeSpecName: "kube-api-access-hqt9s") pod "309fe6fb-d94c-47e6-bba8-badc5d6042a4" (UID: "309fe6fb-d94c-47e6-bba8-badc5d6042a4"). InnerVolumeSpecName "kube-api-access-hqt9s". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 19 08:34:55 minikube kubelet[1570]: I0519 08:34:55.200739    1570 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-hqt9s\" (UniqueName: \"kubernetes.io/projected/309fe6fb-d94c-47e6-bba8-badc5d6042a4-kube-api-access-hqt9s\") on node \"minikube\" DevicePath \"\""
May 19 08:34:55 minikube kubelet[1570]: I0519 08:34:55.638005    1570 scope.go:117] "RemoveContainer" containerID="fac083a57b963319ea7bc7d0ffba56beab1f9c5ae60065ff445307bbbda63ab7"
May 19 08:34:55 minikube kubelet[1570]: I0519 08:34:55.709032    1570 scope.go:117] "RemoveContainer" containerID="fac083a57b963319ea7bc7d0ffba56beab1f9c5ae60065ff445307bbbda63ab7"
May 19 08:34:55 minikube kubelet[1570]: E0519 08:34:55.710422    1570 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: fac083a57b963319ea7bc7d0ffba56beab1f9c5ae60065ff445307bbbda63ab7" containerID="fac083a57b963319ea7bc7d0ffba56beab1f9c5ae60065ff445307bbbda63ab7"
May 19 08:34:55 minikube kubelet[1570]: I0519 08:34:55.710529    1570 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"fac083a57b963319ea7bc7d0ffba56beab1f9c5ae60065ff445307bbbda63ab7"} err="failed to get container status \"fac083a57b963319ea7bc7d0ffba56beab1f9c5ae60065ff445307bbbda63ab7\": rpc error: code = Unknown desc = Error response from daemon: No such container: fac083a57b963319ea7bc7d0ffba56beab1f9c5ae60065ff445307bbbda63ab7"
May 19 08:34:56 minikube kubelet[1570]: I0519 08:34:56.245453    1570 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="309fe6fb-d94c-47e6-bba8-badc5d6042a4" path="/var/lib/kubelet/pods/309fe6fb-d94c-47e6-bba8-badc5d6042a4/volumes"
May 19 08:55:04 minikube kubelet[1570]: I0519 08:55:04.913996    1570 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-g7nr4\" (UniqueName: \"kubernetes.io/projected/7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2-kube-api-access-g7nr4\") pod \"7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2\" (UID: \"7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2\") "
May 19 08:55:04 minikube kubelet[1570]: I0519 08:55:04.917125    1570 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2-kube-api-access-g7nr4" (OuterVolumeSpecName: "kube-api-access-g7nr4") pod "7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2" (UID: "7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2"). InnerVolumeSpecName "kube-api-access-g7nr4". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 19 08:55:05 minikube kubelet[1570]: I0519 08:55:05.014878    1570 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-g7nr4\" (UniqueName: \"kubernetes.io/projected/7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2-kube-api-access-g7nr4\") on node \"minikube\" DevicePath \"\""
May 19 08:55:05 minikube kubelet[1570]: I0519 08:55:05.099666    1570 scope.go:117] "RemoveContainer" containerID="95720f08d6d27dcbb08de598ea08caea6dc3124534717c3d5e77aed83a24ebd1"
May 19 08:55:05 minikube kubelet[1570]: I0519 08:55:05.135534    1570 scope.go:117] "RemoveContainer" containerID="95720f08d6d27dcbb08de598ea08caea6dc3124534717c3d5e77aed83a24ebd1"
May 19 08:55:05 minikube kubelet[1570]: E0519 08:55:05.137788    1570 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 95720f08d6d27dcbb08de598ea08caea6dc3124534717c3d5e77aed83a24ebd1" containerID="95720f08d6d27dcbb08de598ea08caea6dc3124534717c3d5e77aed83a24ebd1"
May 19 08:55:05 minikube kubelet[1570]: I0519 08:55:05.137838    1570 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"95720f08d6d27dcbb08de598ea08caea6dc3124534717c3d5e77aed83a24ebd1"} err="failed to get container status \"95720f08d6d27dcbb08de598ea08caea6dc3124534717c3d5e77aed83a24ebd1\": rpc error: code = Unknown desc = Error response from daemon: No such container: 95720f08d6d27dcbb08de598ea08caea6dc3124534717c3d5e77aed83a24ebd1"
May 19 08:55:06 minikube kubelet[1570]: I0519 08:55:06.270343    1570 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2" path="/var/lib/kubelet/pods/7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2/volumes"
May 19 09:02:56 minikube kubelet[1570]: I0519 09:02:56.369563    1570 topology_manager.go:215] "Topology Admit Handler" podUID="f7f6e59b-2318-432e-a1b0-3f5aaa902526" podNamespace="argo-rollouts" podName="argo-rollouts-dashboard-5577ff8545-xv4x7"
May 19 09:02:56 minikube kubelet[1570]: E0519 09:02:56.372026    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 19 09:02:56 minikube kubelet[1570]: E0519 09:02:56.372513    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 19 09:02:56 minikube kubelet[1570]: E0519 09:02:56.372896    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="309fe6fb-d94c-47e6-bba8-badc5d6042a4" containerName="argo-rollouts-dashboard"
May 19 09:02:56 minikube kubelet[1570]: E0519 09:02:56.373321    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2" containerName="argo-rollouts-dashboard"
May 19 09:02:56 minikube kubelet[1570]: I0519 09:02:56.374029    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 19 09:02:56 minikube kubelet[1570]: I0519 09:02:56.374405    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="309fe6fb-d94c-47e6-bba8-badc5d6042a4" containerName="argo-rollouts-dashboard"
May 19 09:02:56 minikube kubelet[1570]: I0519 09:02:56.374756    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 19 09:02:56 minikube kubelet[1570]: I0519 09:02:56.375145    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="7bcbf5f9-7637-41bd-8fa6-5306af0bb3e2" containerName="argo-rollouts-dashboard"
May 19 09:02:56 minikube kubelet[1570]: I0519 09:02:56.548528    1570 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8fhjf\" (UniqueName: \"kubernetes.io/projected/f7f6e59b-2318-432e-a1b0-3f5aaa902526-kube-api-access-8fhjf\") pod \"argo-rollouts-dashboard-5577ff8545-xv4x7\" (UID: \"f7f6e59b-2318-432e-a1b0-3f5aaa902526\") " pod="argo-rollouts/argo-rollouts-dashboard-5577ff8545-xv4x7"
May 19 09:18:55 minikube kubelet[1570]: I0519 09:18:55.010411    1570 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="argo-rollouts/argo-rollouts-dashboard-5577ff8545-xv4x7" podStartSLOduration=954.3976728 podCreationTimestamp="2024-05-19 09:02:56 +0000 UTC" firstStartedPulling="2024-05-19 09:02:57.271131777 +0000 UTC m=+15239.790649756" lastFinishedPulling="2024-05-19 09:03:01.883823601 +0000 UTC m=+15244.403341657" observedRunningTime="2024-05-19 09:03:03.049811009 +0000 UTC m=+15245.569328980" watchObservedRunningTime="2024-05-19 09:18:55.010364701 +0000 UTC m=+16197.529882656"
May 19 09:18:55 minikube kubelet[1570]: I0519 09:18:55.550557    1570 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-8fhjf\" (UniqueName: \"kubernetes.io/projected/f7f6e59b-2318-432e-a1b0-3f5aaa902526-kube-api-access-8fhjf\") pod \"f7f6e59b-2318-432e-a1b0-3f5aaa902526\" (UID: \"f7f6e59b-2318-432e-a1b0-3f5aaa902526\") "
May 19 09:18:55 minikube kubelet[1570]: I0519 09:18:55.559262    1570 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f7f6e59b-2318-432e-a1b0-3f5aaa902526-kube-api-access-8fhjf" (OuterVolumeSpecName: "kube-api-access-8fhjf") pod "f7f6e59b-2318-432e-a1b0-3f5aaa902526" (UID: "f7f6e59b-2318-432e-a1b0-3f5aaa902526"). InnerVolumeSpecName "kube-api-access-8fhjf". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 19 09:18:55 minikube kubelet[1570]: I0519 09:18:55.651733    1570 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-8fhjf\" (UniqueName: \"kubernetes.io/projected/f7f6e59b-2318-432e-a1b0-3f5aaa902526-kube-api-access-8fhjf\") on node \"minikube\" DevicePath \"\""
May 19 09:18:55 minikube kubelet[1570]: I0519 09:18:55.972697    1570 scope.go:117] "RemoveContainer" containerID="c123f20d9906ff6322da32e6e8fa500f5638c6b960dbcbbfc8f59a4acbc9ba0b"
May 19 09:18:56 minikube kubelet[1570]: I0519 09:18:56.010275    1570 scope.go:117] "RemoveContainer" containerID="c123f20d9906ff6322da32e6e8fa500f5638c6b960dbcbbfc8f59a4acbc9ba0b"
May 19 09:18:56 minikube kubelet[1570]: E0519 09:18:56.012109    1570 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: c123f20d9906ff6322da32e6e8fa500f5638c6b960dbcbbfc8f59a4acbc9ba0b" containerID="c123f20d9906ff6322da32e6e8fa500f5638c6b960dbcbbfc8f59a4acbc9ba0b"
May 19 09:18:56 minikube kubelet[1570]: I0519 09:18:56.012157    1570 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"c123f20d9906ff6322da32e6e8fa500f5638c6b960dbcbbfc8f59a4acbc9ba0b"} err="failed to get container status \"c123f20d9906ff6322da32e6e8fa500f5638c6b960dbcbbfc8f59a4acbc9ba0b\": rpc error: code = Unknown desc = Error response from daemon: No such container: c123f20d9906ff6322da32e6e8fa500f5638c6b960dbcbbfc8f59a4acbc9ba0b"
May 19 09:18:56 minikube kubelet[1570]: I0519 09:18:56.274046    1570 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="f7f6e59b-2318-432e-a1b0-3f5aaa902526" path="/var/lib/kubelet/pods/f7f6e59b-2318-432e-a1b0-3f5aaa902526/volumes"
May 19 09:19:19 minikube kubelet[1570]: I0519 09:19:19.561732    1570 topology_manager.go:215] "Topology Admit Handler" podUID="da803f8e-8694-4f7a-9c95-749af8611130" podNamespace="default" podName="argo-rollouts-dashboard-5577ff8545-tr74x"
May 19 09:19:19 minikube kubelet[1570]: E0519 09:19:19.561956    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 19 09:19:19 minikube kubelet[1570]: E0519 09:19:19.561987    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8bd37b7c-3832-4385-82ad-8fa22fa0ca4c" containerName="load-gen"
May 19 09:19:19 minikube kubelet[1570]: E0519 09:19:19.562010    1570 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f7f6e59b-2318-432e-a1b0-3f5aaa902526" containerName="argo-rollouts-dashboard"
May 19 09:19:19 minikube kubelet[1570]: I0519 09:19:19.562111    1570 memory_manager.go:346] "RemoveStaleState removing state" podUID="f7f6e59b-2318-432e-a1b0-3f5aaa902526" containerName="argo-rollouts-dashboard"
May 19 09:19:19 minikube kubelet[1570]: I0519 09:19:19.678609    1570 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-b2nbp\" (UniqueName: \"kubernetes.io/projected/da803f8e-8694-4f7a-9c95-749af8611130-kube-api-access-b2nbp\") pod \"argo-rollouts-dashboard-5577ff8545-tr74x\" (UID: \"da803f8e-8694-4f7a-9c95-749af8611130\") " pod="default/argo-rollouts-dashboard-5577ff8545-tr74x"
May 19 09:19:20 minikube kubelet[1570]: I0519 09:19:20.228306    1570 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6744c5d6fbd8797a9b792e2ca2ba7940c2172c34bbf7d1a29acfb6e3b7b63958"

* 
* ==> storage-provisioner [50bbeced86bc] <==
* I0517 20:43:40.292114       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0517 20:44:10.432866       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [7154bc9d26e7] <==
* I0517 20:45:05.285665       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0517 20:45:05.307998       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0517 20:45:05.308758       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0517 20:45:22.724706       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0517 20:45:22.724927       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_1a2ef439-0a88-429c-8709-8b6d6547cffe!
I0517 20:45:22.728738       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"ca1485c6-7f62-4277-a7f7-5ded017a1d26", APIVersion:"v1", ResourceVersion:"851974", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_1a2ef439-0a88-429c-8709-8b6d6547cffe became leader
I0517 20:45:22.825568       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_1a2ef439-0a88-429c-8709-8b6d6547cffe!

